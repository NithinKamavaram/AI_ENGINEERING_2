{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJXW_DgiSebM"
      },
      "source": [
        "# LangGraph and LangSmith - Agentic RAG Powered by LangChain\n",
        "\n",
        "In the following notebook we'll complete the following tasks:\n",
        "\n",
        "- ü§ù Breakout Room #1:\n",
        "  1. Install required libraries\n",
        "  2. Set Environment Variables\n",
        "  3. Creating our Tool Belt\n",
        "  4. Creating Our State\n",
        "  5. Creating and Compiling A Graph!\n",
        "  \n",
        "- ü§ù Breakout Room #2:\n",
        "  - Part 1:\n",
        "    1. Creating an Evaluation Dataset\n",
        "    2. Adding Evaluators\n",
        "    3. Evaluating\n",
        "  - Part 2:\n",
        "    1. Adding conditional check."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djQ3nRAgoF67"
      },
      "source": [
        "# ü§ù Breakout Room #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7pQDUhUnIo8"
      },
      "source": [
        "## Part 1: LangGraph - Building Cyclic Applications with LangChain\n",
        "\n",
        "LangGraph is a tool that leverages LangChain Expression Language to build coordinated multi-actor and stateful applications that includes cyclic behaviour.\n",
        "\n",
        "### Why Cycles?\n",
        "\n",
        "In essence, we can think of a cycle in our graph as a more robust and customizable loop. It allows us to keep our application agent-forward while still giving the powerful functionality of traditional loops.\n",
        "\n",
        "Due to the inclusion of cycles over loops, we can also compose rather complex flows through our graph in a much more readable and natural fashion. Effetively allowing us to recreate appliation flowcharts in code in an almost 1-to-1 fashion.\n",
        "\n",
        "### Why LangGraph?\n",
        "\n",
        "Beyond the agent-forward approach - we can easily compose and combine traditional \"DAG\" (directed acyclic graph) chains with powerful cyclic behaviour due to the tight integration with LCEL. This means it's a natural extension to LangChain's core offerings!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_fLDElOVoop"
      },
      "source": [
        "## Task 1:  Dependencies\n",
        "\n",
        "We'll first install all our required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaVwN269EttM",
        "outputId": "d2b0dd31-273b-4f63-b019-50e7eddd15b1"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain langchain_openai langgraph arxiv duckduckgo-search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wujPjGJuoPwg"
      },
      "source": [
        "## Task 2: Environment Variables\n",
        "\n",
        "We'll want to set both our OpenAI API key and our LangSmith environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdh8CoVWHRvs",
        "outputId": "db73ce2a-b96e-475c-df1a-374ecf32fdef"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv0glIDyHmRt",
        "outputId": "82647f16-a782-48eb-c0e5-568b8896cdc6"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE2 - LangGraph - {uuid4().hex[0:8]}\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBRyQmEAVzua"
      },
      "source": [
        "## Task 3: Creating our Tool Belt\n",
        "\n",
        "As is usually the case, we'll want to equip our agent with a toolbelt to help answer questions and add external knowledge.\n",
        "\n",
        "There's a tonne of tools in the [LangChain Community Repo](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools) but we'll stick to a couple just so we can observe the cyclic nature of LangGraph in action!\n",
        "\n",
        "We'll leverage:\n",
        "\n",
        "- [Duck Duck Go Web Search](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/ddg_search)\n",
        "- [Arxiv](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/arxiv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k6n_Dob2F46"
      },
      "source": [
        "####üèóÔ∏è Activity #1:\n",
        "\n",
        "Please add the tools to use into our toolbelt.\n",
        "\n",
        "> NOTE: Each tool in our toolbelt should be a method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lAxaSvlfIeOg"
      },
      "outputs": [],
      "source": [
        "from langchain_community.tools.ddg_search import DuckDuckGoSearchRun\n",
        "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
        "\n",
        "tool_belt = [\n",
        "    DuckDuckGoSearchRun(),\n",
        "    ArxivQueryRun()\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FdOjEslXdRR"
      },
      "source": [
        "### Actioning with Tools\n",
        "\n",
        "Now that we've created our tool belt - we need to create a process that will let us leverage them when we need them.\n",
        "\n",
        "We'll use the built-in [`ToolExecutor`](https://github.com/langchain-ai/langgraph/blob/main/langgraph/prebuilt/tool_executor.py) to do so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cFr1m80-JZsD"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolExecutor\n",
        "\n",
        "tool_executor = ToolExecutor(tool_belt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[DuckDuckGoSearchRun(), ArxivQueryRun()]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tool_executor.tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI-C669ZYVI5"
      },
      "source": [
        "### Model\n",
        "\n",
        "Now we can set-up our model! We'll leverage the familiar OpenAI model suite for this example - but it's not *necessary* to use with LangGraph. LangGraph supports all models - though you might not find success with smaller models - as such, they recommend you stick with:\n",
        "\n",
        "- OpenAI's GPT-3.5 and GPT-4\n",
        "- Anthropic's Claude\n",
        "- Google's Gemini\n",
        "\n",
        "> NOTE: Because we're leveraging the OpenAI function calling API - we'll need to use OpenAI *for this specific example* (or any other service that exposes an OpenAI-style function calling API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QkNS8rNZJs4z"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugkj3GzuZpQv"
      },
      "source": [
        "Now that we have our model set-up, let's \"put on the tool belt\", which is to say: We'll bind our LangChain formatted tools to the model in an OpenAI function calling format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "4OdMqFafZ_0V"
      },
      "outputs": [],
      "source": [
        "from langchain_core.utils.function_calling import convert_to_openai_function\n",
        "\n",
        "functions = [convert_to_openai_function(t) for t in tool_belt]\n",
        "model = model.bind_functions(functions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'name': 'duckduckgo_search',\n",
              "  'description': 'A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query.',\n",
              "  'parameters': {'type': 'object',\n",
              "   'properties': {'query': {'description': 'search query to look up',\n",
              "     'type': 'string'}},\n",
              "   'required': ['query']}},\n",
              " {'name': 'arxiv',\n",
              "  'description': 'A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.',\n",
              "  'parameters': {'type': 'object',\n",
              "   'properties': {'query': {'description': 'search query to look up',\n",
              "     'type': 'string'}},\n",
              "   'required': ['query']}}]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.kwargs['functions']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERzuGo6W18Lr"
      },
      "source": [
        "#### ‚ùì Question #1:\n",
        "\n",
        "How does the model determine which tool to use?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**ANSWER**:\n",
        "- Based on tool/function description, we can find using below code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tool: duckduckgo_search\n",
            "description: A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query.\n",
            "********\n",
            "tool: arxiv\n",
            "description: A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.\n",
            "********\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(model.kwargs['functions'])):\n",
        "    print('tool: ' + model.kwargs['functions'][i]['name'])\n",
        "    print('description: ' + model.kwargs['functions'][i]['description'])\n",
        "    print(\"********\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_296Ub96Z_H8"
      },
      "source": [
        "## Task 4: Putting the State in Stateful\n",
        "\n",
        "Earlier we used this phrasing:\n",
        "\n",
        "`coordinated multi-actor and stateful applications`\n",
        "\n",
        "So what does that \"stateful\" mean?\n",
        "\n",
        "To put it simply - we want to have some kind of object which we can pass around our application that holds information about what the current situation (state) is. Since our system will be constructed of many parts moving in a coordinated fashion - we want to be able to ensure we have some commonly understood idea of that state.\n",
        "\n",
        "LangGraph leverages a `StatefulGraph` which uses an `AgentState` object to pass information between the various nodes of the graph.\n",
        "\n",
        "There are more options than what we'll see below - but this `AgentState` object is one that is stored in a `TypedDict` with the key `messages` and the value is a `Sequence` of `BaseMessages` that will be appended to whenever the state changes.\n",
        "\n",
        "Let's think about a simple example to help understand exactly what this means (we'll simplify a great deal to try and clearly communicate what state is doing):\n",
        "\n",
        "1. We initialize our state object:\n",
        "  - `{\"messages\" : []}`\n",
        "2. Our user submits a query to our application.\n",
        "  - New State: `HumanMessage(#1)`\n",
        "  - `{\"messages\" : [HumanMessage(#1)}`\n",
        "3. We pass our state object to an Agent node which is able to read the current state. It will use the last `HumanMessage` as input. It gets some kind of output which it will add to the state.\n",
        "  - New State: `AgentMessage(#1, additional_kwargs {\"function_call\" : \"WebSearchTool\"})`\n",
        "  - `{\"messages\" : [HumanMessage(#1), AgentMessage(#1, ...)]}`\n",
        "4. We pass our state object to a \"conditional node\" (more on this later) which reads the last state to determine if we need to use a tool - which it can determine properly because of our provided object!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "mxL9b_NZKUdL"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "import operator\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWsMhfO9grLu"
      },
      "source": [
        "## Task 5: It's Graphing Time!\n",
        "\n",
        "Now that we have state, and we have tools, and we have an LLM - we can finally start making our graph!\n",
        "\n",
        "Let's take a second to refresh ourselves about what a graph is in this context.\n",
        "\n",
        "Graphs, also called networks in some circles, are a collection of connected objects.\n",
        "\n",
        "The objects in question are typically called nodes, or vertices, and the connections are called edges.\n",
        "\n",
        "Let's look at a simple graph.\n",
        "\n",
        "![image](https://i.imgur.com/2NFLnIc.png)\n",
        "\n",
        "Here, we're using the coloured circles to represent the nodes and the yellow lines to represent the edges. In this case, we're looking at a fully connected graph - where each node is connected by an edge to each other node.\n",
        "\n",
        "If we were to think about nodes in the context of LangGraph - we would think of a function, or an LCEL runnable.\n",
        "\n",
        "If we were to think about edges in the context of LangGraph - we might think of them as \"paths to take\" or \"where to pass our state object next\".\n",
        "\n",
        "Let's create some nodes and expand on our diagram.\n",
        "\n",
        "> NOTE: Due to the tight integration with LCEL - we can comfortably create our nodes in an async fashion!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "91flJWtZLUrl"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolInvocation\n",
        "import json\n",
        "from langchain_core.messages import FunctionMessage\n",
        "\n",
        "def call_model(state):\n",
        "  messages = state[\"messages\"]\n",
        "  response = model.invoke(messages)\n",
        "  return {\"messages\" : [response]}\n",
        "\n",
        "def call_tool(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  action = ToolInvocation(\n",
        "      tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n",
        "      tool_input=json.loads(\n",
        "          last_message.additional_kwargs[\"function_call\"][\"arguments\"]\n",
        "      )\n",
        "  )\n",
        "\n",
        "  response = tool_executor.invoke(action)\n",
        "\n",
        "  function_message = FunctionMessage(content=str(response), name=action.tool)\n",
        "\n",
        "  return {\"messages\" : [function_message]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bwR7MgWj3Wg"
      },
      "source": [
        "Now we have two total nodes. We have:\n",
        "\n",
        "- `call_model` is a node that will...well...call the model\n",
        "- `call_tool` is a node which will call a tool\n",
        "\n",
        "Let's start adding nodes! We'll update our diagram along the way to keep track of what this looks like!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "_vF4_lgtmQNo"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "workflow.add_node(\"agent\", call_model)\n",
        "workflow.add_node(\"action\", call_tool)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8CjRlbVmRpW"
      },
      "source": [
        "Let's look at what we have so far:\n",
        "\n",
        "![image](https://i.imgur.com/md7inqG.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaXHpPeSnOWC"
      },
      "source": [
        "Next, we'll add our entrypoint. All our entrypoint does is indicate which node is called first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "YGCbaYqRnmiw"
      },
      "outputs": [],
      "source": [
        "workflow.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUsfGoSpoF9U"
      },
      "source": [
        "![image](https://i.imgur.com/wNixpJe.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q_pQgHmoW0M"
      },
      "source": [
        "Now we want to build a \"conditional edge\" which will use the output state of a node to determine which path to follow.\n",
        "\n",
        "We can help conceptualize this by thinking of our conditional edge as a conditional in a flowchart!\n",
        "\n",
        "Notice how our function simply checks if there is a \"function_call\" kwarg present.\n",
        "\n",
        "Then we create an edge where the origin node is our agent node and our destination node is *either* the action node or the END (finish the graph).\n",
        "\n",
        "It's important to highlight that the dictionary passed in as the third parameter (the mapping) should be created with the possible outputs of our conditional function in mind. In this case `should_continue` outputs either `\"end\"` or `\"continue\"` which are subsequently mapped to the action node or the END node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "1BZgb81VQf9o"
      },
      "outputs": [],
      "source": [
        "def should_continue(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if \"function_call\" not in last_message.additional_kwargs:\n",
        "    return \"end\"\n",
        "\n",
        "  return \"continue\"\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\" : \"action\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Cvhcf4jp0Ce"
      },
      "source": [
        "Let's visualize what this looks like.\n",
        "\n",
        "![image](https://i.imgur.com/8ZNwKI5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKCjWJCkrJb9"
      },
      "source": [
        "Finally, we can add our last edge which will connect our action node to our agent node. This is because we *always* want our action node (which is used to call our tools) to return its output to our agent!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "UvcgbHf1rIXZ"
      },
      "outputs": [],
      "source": [
        "workflow.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiWDwBQtrw7Z"
      },
      "source": [
        "Let's look at the final visualization.\n",
        "\n",
        "![image](https://i.imgur.com/NWO7usO.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYqDpErlsCsu"
      },
      "source": [
        "All that's left to do now is to compile our workflow - and we're off!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{('__start__', 'agent'), ('action', 'agent')}"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "workflow.edges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "zt9-KS8DpzNx"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CompiledStateGraph(nodes={'__start__': PregelNode(config={'tags': ['langsmith:hidden']}, channels=['__start__'], triggers=['__start__'], writers=[ChannelWrite<messages>(recurse=True, writes=[ChannelWriteEntry(channel='messages', value=<object object at 0x10a24e550>, skip_none=False, mapper=_get_state_key(recurse=False))]), ChannelWrite<start:agent>(recurse=True, writes=[ChannelWriteEntry(channel='start:agent', value='__start__', skip_none=False, mapper=None)])]), 'agent': PregelNode(config={'tags': []}, channels={'messages': 'messages'}, triggers=['action', 'start:agent'], mapper=functools.partial(<function _coerce_state at 0x10ea35da0>, <class '__main__.AgentState'>), writers=[ChannelWrite<agent,messages>(recurse=True, writes=[ChannelWriteEntry(channel='agent', value='agent', skip_none=False, mapper=None), ChannelWriteEntry(channel='messages', value=<object object at 0x10a24e550>, skip_none=False, mapper=_get_state_key(recurse=False))]), _route(recurse=True, _is_channel_writer=True)]), 'action': PregelNode(config={'tags': []}, channels={'messages': 'messages'}, triggers=['branch:agent:should_continue:action'], mapper=functools.partial(<function _coerce_state at 0x10ea35da0>, <class '__main__.AgentState'>), writers=[ChannelWrite<action,messages>(recurse=True, writes=[ChannelWriteEntry(channel='action', value='action', skip_none=False, mapper=None), ChannelWriteEntry(channel='messages', value=<object object at 0x10a24e550>, skip_none=False, mapper=_get_state_key(recurse=False))])])}, channels={'messages': <langgraph.channels.binop.BinaryOperatorAggregate object at 0x128b8da90>, '__start__': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x128b6a890>, 'agent': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x12826d6d0>, 'action': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x10ffb9010>, 'start:agent': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x128b6a290>, 'branch:agent:should_continue:action': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x128b62b10>}, auto_validate=False, stream_mode='updates', output_channels=['messages'], stream_channels=['messages'], input_channels='__start__', builder=<langgraph.graph.state.StateGraph object at 0x128b8d910>)"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "app = workflow.compile()\n",
        "app"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhNWIwBL1W4Q"
      },
      "source": [
        "#### ‚ùì Question #2:\n",
        "\n",
        "Is there any specific limit to how many times we can cycle?\n",
        "\n",
        "If not, how could we impose a limit to the number of cycles?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**ANSWER**:\n",
        "- There is no specific limit on how many times we can cycle by default, but we can add some condition logic to limit the number of cycles to specific number based on number of states it generated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSCds6zTL5VJ"
      },
      "source": [
        "#### Helper Function to print messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "xRPF0X5iL8Bh"
      },
      "outputs": [],
      "source": [
        "def print_messages(messages):\n",
        "  next_is_tool = False\n",
        "  initial_query = True\n",
        "  for message in messages[\"messages\"]:\n",
        "    if \"function_call\" in message.additional_kwargs:\n",
        "      print()\n",
        "      print(f'Tool Call - Name: {message.additional_kwargs[\"function_call\"][\"name\"]} + Query: {message.additional_kwargs[\"function_call\"][\"arguments\"]}')\n",
        "      next_is_tool = True\n",
        "      continue\n",
        "    if next_is_tool:\n",
        "      print(f\"Tool Response: {message.content}\")\n",
        "      next_is_tool = False\n",
        "      continue\n",
        "    if initial_query:\n",
        "      print(f\"Initial Query: {message.content}\")\n",
        "      print()\n",
        "      initial_query = False\n",
        "      continue\n",
        "    print()\n",
        "    print(f\"Agent Response: {message.content}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEYcTShCsPaa"
      },
      "source": [
        "## Using Our Graph\n",
        "\n",
        "Now that we've created and compiled our graph - we can call it *just as we'd call any other* `Runnable`!\n",
        "\n",
        "Let's try out a few examples to see how it fairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CompiledStateGraph(nodes={'__start__': PregelNode(config={'tags': ['langsmith:hidden']}, channels=['__start__'], triggers=['__start__'], writers=[ChannelWrite<messages>(recurse=True, writes=[ChannelWriteEntry(channel='messages', value=<object object at 0x10a24e550>, skip_none=False, mapper=_get_state_key(recurse=False))]), ChannelWrite<start:agent>(recurse=True, writes=[ChannelWriteEntry(channel='start:agent', value='__start__', skip_none=False, mapper=None)])]), 'agent': PregelNode(config={'tags': []}, channels={'messages': 'messages'}, triggers=['action', 'start:agent'], mapper=functools.partial(<function _coerce_state at 0x10ea35da0>, <class '__main__.AgentState'>), writers=[ChannelWrite<agent,messages>(recurse=True, writes=[ChannelWriteEntry(channel='agent', value='agent', skip_none=False, mapper=None), ChannelWriteEntry(channel='messages', value=<object object at 0x10a24e550>, skip_none=False, mapper=_get_state_key(recurse=False))]), _route(recurse=True, _is_channel_writer=True)]), 'action': PregelNode(config={'tags': []}, channels={'messages': 'messages'}, triggers=['branch:agent:should_continue:action'], mapper=functools.partial(<function _coerce_state at 0x10ea35da0>, <class '__main__.AgentState'>), writers=[ChannelWrite<action,messages>(recurse=True, writes=[ChannelWriteEntry(channel='action', value='action', skip_none=False, mapper=None), ChannelWriteEntry(channel='messages', value=<object object at 0x10a24e550>, skip_none=False, mapper=_get_state_key(recurse=False))])])}, channels={'messages': <langgraph.channels.binop.BinaryOperatorAggregate object at 0x128b8da90>, '__start__': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x128b6a890>, 'agent': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x12826d6d0>, 'action': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x10ffb9010>, 'start:agent': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x128b6a290>, 'branch:agent:should_continue:action': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x128b62b10>}, auto_validate=False, stream_mode='updates', output_channels=['messages'], stream_channels=['messages'], input_channels='__start__', builder=<langgraph.graph.state.StateGraph object at 0x128b8d910>)"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "app"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn4n37PQRPII",
        "outputId": "db0f32d3-724a-4f64-9a48-121ee2ddfb0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Query: What is RAG in the context of Large Language Models? When did it break onto the scene?\n",
            "\n",
            "\n",
            "Tool Call - Name: duckduckgo_search + Query: {\"query\":\"RAG in the context of Large Language Models\"}\n",
            "Tool Response: Large language models (LLMs) are incredibly powerful tools for processing and generating text. However, they inherently struggle to understand the broader context of information, especially when dealing with lengthy conversations or complex tasks. This is where large context windows and Retrieval-Augmented Generation (RAG) come into play. February 2024 marked a pivotal moment in the AI field as Google unveiled Gemini 1.5 Pro, boasting an impressive 1M tokens of context capacity.This advance, closely following OpenAI's \"turbo\" enhancement to GPT-4 and Anthropic's Claude 2.1, has reignited the discourse on the relevance of Retrieval-Augmented Generation (RAG) in an era dominated by Large Context Models. Using RAG 2.0, we've created our first set of Contextual Language Models (CLMs), which achieve state-of-the-art performance on a wide variety of industry benchmarks. CLMs outperform strong RAG baselines based on GPT-4 and the best open-source models by a large margin, according to our research and our customers. Retrieval-augmented generation (RAG) is a technique used to \"ground\" large language models (LLMs) with specific data sources, often sources that weren't included in the models' original ... The field of natural language processing has been revolutionized by large language models (LLMs), which showcase advanced capabilities and‚Ä¶ 15 min read ¬∑ Jan 25, 2024 14\n",
            "\n",
            "Agent Response: Retrieval-Augmented Generation (RAG) is a technique used in the context of Large Language Models (LLMs) to \"ground\" the models with specific data sources, often sources that were not included in the models' original training data. RAG helps LLMs understand broader contexts of information, especially in lengthy conversations or complex tasks.\n",
            "\n",
            "RAG broke onto the scene in February 2024 when Google unveiled Gemini 1.5 Pro, which boasted an impressive 1 million tokens of context capacity. This advance in the AI field reignited the discourse on the relevance of RAG in an era dominated by Large Context Models.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "inputs = {\"messages\" : [HumanMessage(content=\"What is RAG in the context of Large Language Models? When did it break onto the scene?\")]}\n",
        "\n",
        "messages = app.invoke(inputs)\n",
        "\n",
        "print_messages(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBHnUtLSscRr"
      },
      "source": [
        "Let's look at what happened:\n",
        "\n",
        "1. Our state object was populated with our request\n",
        "2. The state object was passed into our entry point (agent node) and the agent node added an `AIMessage` to the state object and passed it along the conditional edge\n",
        "3. The conditional edge received the state object, found the \"function_call\" `additional_kwarg`, and sent the state object to the action node\n",
        "4. The action node added the response from the OpenAI function calling endpoint to the state object and passed it along the edge to the agent node\n",
        "5. The agent node added a response to the state object and passed it along the conditional edge\n",
        "6. The conditional edge received the state object, could not find the \"function_call\" `additional_kwarg` and passed the state object to END where we see it output in the cell above!\n",
        "\n",
        "Now let's look at an example that shows a multiple tool usage - all with the same flow!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afv2BuEsV5JG",
        "outputId": "88063264-fdef-4bb3-a7c5-3a3eb1a1cff3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Query: What is QLoRA in Machine Learning? Are their any technical papers that could help me understand? Once you have that information, can you look up the bio of the first author on the QLoRA paper?\n",
            "\n",
            "\n",
            "Tool Call - Name: arxiv + Query: {\"query\":\"QLoRA in Machine Learning\"}\n",
            "Tool Response: Published: 2023-05-23\n",
            "Title: QLoRA: Efficient Finetuning of Quantized LLMs\n",
            "Authors: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\n",
            "Summary: We present QLoRA, an efficient finetuning approach that reduces memory usage\n",
            "enough to finetune a 65B parameter model on a single 48GB GPU while preserving\n",
            "full 16-bit finetuning task performance. QLoRA backpropagates gradients through\n",
            "a frozen, 4-bit quantized pretrained language model into Low Rank\n",
            "Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all\n",
            "previous openly released models on the Vicuna benchmark, reaching 99.3% of the\n",
            "performance level of ChatGPT while only requiring 24 hours of finetuning on a\n",
            "single GPU. QLoRA introduces a number of innovations to save memory without\n",
            "sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is\n",
            "information theoretically optimal for normally distributed weights (b) double\n",
            "quantization to reduce the average memory footprint by quantizing the\n",
            "quantization constants, and (c) paged optimziers to manage memory spikes. We\n",
            "use QLoRA to finetune more than 1,000 models, providing a detailed analysis of\n",
            "instruction following and chatbot performance across 8 instruction datasets,\n",
            "multiple model types (LLaMA, T5), and model scales that would be infeasible to\n",
            "run with regular finetuning (e.g. 33B and 65B parameter models). Our results\n",
            "show that QLoRA finetuning on a small high-quality dataset leads to\n",
            "state-of-the-art results, even when using smaller models than the previous\n",
            "SoTA. We provide a detailed analysis of chatbot performance based on both human\n",
            "and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable\n",
            "alternative to human evaluation. Furthermore, we find that current chatbot\n",
            "benchmarks are not trustworthy to accurately evaluate the performance levels of\n",
            "chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to\n",
            "ChatGPT. We release all of our models and code, including CUDA kernels for\n",
            "4-bit training.\n",
            "\n",
            "Published: 2023-12-31\n",
            "Title: Viz: A QLoRA-based Copyright Marketplace for Legally Compliant Generative AI\n",
            "Authors: Dipankar Sarkar\n",
            "Summary: This paper aims to introduce and analyze the Viz system in a comprehensive\n",
            "way, a novel system architecture that integrates Quantized Low-Rank Adapters\n",
            "(QLoRA) to fine-tune large language models (LLM) within a legally compliant and\n",
            "resource efficient marketplace. Viz represents a significant contribution to\n",
            "the field of artificial intelligence, particularly in addressing the challenges\n",
            "of computational efficiency, legal compliance, and economic sustainability in\n",
            "the utilization and monetization of LLMs. The paper delineates the scholarly\n",
            "discourse and developments that have informed the creation of Viz, focusing\n",
            "primarily on the advancements in LLM models, copyright issues in AI training\n",
            "(NYT case, 2023), and the evolution of model fine-tuning techniques,\n",
            "particularly low-rank adapters and quantized low-rank adapters, to create a\n",
            "sustainable and economically compliant framework for LLM utilization. The\n",
            "economic model it proposes benefits content creators, AI developers, and\n",
            "end-users, delineating a harmonious integration of technology, economy, and\n",
            "law, offering a comprehensive solution to the complex challenges of today's AI\n",
            "landscape.\n",
            "\n",
            "Published: 2024-02-08\n",
            "Title: Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\n",
            "Authors: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno\n",
            "Summary: The LoRA-finetuning quantization of LLMs has been extensively studied to\n",
            "obtain accurate yet compact LLMs for deployment on resource-constrained\n",
            "hardware. However, existing methods cause the quantized LLM to severely degrade\n",
            "and even fail to benefit from the finetuning of LoRA. This paper proposes a\n",
            "novel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate\n",
            "through information retention. The proposed IR-QLoRA mainly relies on two\n",
            "technolog\n",
            "\n",
            "Tool Call - Name: duckduckgo_search + Query: {\"query\":\"Tim Dettmers bio\"}\n",
            "Tool Response: Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. ... Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. ... Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. ... Speaker Biography. Tim Dettmers' research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. ...\n",
            "\n",
            "Agent Response: QLoRA (Quantized Low-Rank Adapters) is an efficient finetuning approach in machine learning that reduces memory usage enough to finetune large language models on a single GPU while preserving performance. It involves backpropagating gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA).\n",
            "\n",
            "Here are some technical papers related to QLoRA:\n",
            "1. **QLoRA: Efficient Finetuning of Quantized LLMs**\n",
            "   - Authors: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\n",
            "   - Summary: Introduces QLoRA, an efficient finetuning approach that reduces memory usage for large language models.\n",
            "   \n",
            "2. **Viz: A QLoRA-based Copyright Marketplace for Legally Compliant Generative AI**\n",
            "   - Author: Dipankar Sarkar\n",
            "   - Summary: Introduces the Viz system, which integrates Quantized Low-Rank Adapters (QLoRA) for fine-tuning large language models within a legally compliant marketplace.\n",
            "\n",
            "3. **Accurate LoRA-Finetuning Quantization of LLMs via Information Retention**\n",
            "   - Authors: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno\n",
            "   - Summary: Proposes IR-QLoRA for accurate and compact quantization of large language models.\n",
            "\n",
            "The first author of the QLoRA paper is Tim Dettmers. Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning.\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"What is QLoRA in Machine Learning? Are their any technical papers that could help me understand? Once you have that information, can you look up the bio of the first author on the QLoRA paper?\")]}\n",
        "\n",
        "messages = app.invoke(inputs)\n",
        "\n",
        "print_messages(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXzDlZVz1Hnf"
      },
      "source": [
        "####üèóÔ∏è Activity #2:\n",
        "\n",
        "Please write out the steps the agent took to arrive at the correct answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**ANSWER**:\n",
        "- Question is passed to agent node as state object, and agent node decides the tool to be used based on questiona and tool description.\n",
        "- the state information is paased to condition edge \"should_continue\", next it checks for function call in additional kwargs anf will continue to action node.\n",
        "- Action node performs the required action and retriever the information needed.\n",
        "- next this is passed back to agent to process the state adn information, and to answer next question \"Tim Dettmers bio\", again agent passes stae to condition edge which continues to action node.\n",
        "- process repeats and agent gets response again, now by this time agent node will have all the respones and llm will be used to give final response.\n",
        "- Finally it checks for condition edge again and this time it didn't find function call and processes to end."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQmrzYfrm1Dr"
      },
      "source": [
        "# ü§ù Breakout Room #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV3XeFOT1Sar"
      },
      "source": [
        "### Pre-processing for LangSmith"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wruQCuzewUuO"
      },
      "source": [
        "To do a little bit more preprocessing, let's wrap our LangGraph agent in a simple chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "oeXdQgbxwhTv"
      },
      "outputs": [],
      "source": [
        "def convert_inputs(input_object):\n",
        "  return {\"messages\" : [HumanMessage(content=input_object[\"question\"])]}\n",
        "\n",
        "def parse_output(input_state):\n",
        "  return input_state[\"messages\"][-1].content\n",
        "\n",
        "agent_chain = convert_inputs | app | parse_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "orYxBZXSxJjZ",
        "outputId": "6eda06b2-0110-44c0-8106-b1280376a2c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAG stands for Retrieval Augmented Generation. It is an AI framework for improving the quality of language models' generated responses by grounding the model on external sources of knowledge to supplement the model's internal representation of information. RAG combines two kinds of models: retrieval of relevant information and generation of text from that information. By using both together, RAG enhances the accuracy and reliability of generative AI models with facts fetched from external sources.\n"
          ]
        }
      ],
      "source": [
        "print(agent_chain.invoke({\"question\" : \"What is RAG?\"}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9UkCIqkpyZu"
      },
      "source": [
        "## Task 1: Creating An Evaluation Dataset\n",
        "\n",
        "Just as we saw last week, we'll want to create a dataset to test our Agent's ability to answer questions.\n",
        "\n",
        "In order to do this - we'll want to provide some questions and some answers. Let's look at how we can create such a dataset below.\n",
        "\n",
        "```python\n",
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"paged\", \"optimizer\"]},\n",
        "    {\"must_mention\" : [\"NF4\", \"NormalFloat\"]},\n",
        "    {\"must_mention\" : [\"ground\", \"context\"]},\n",
        "    {\"must_mention\" : [\"Tim\", \"Dettmers\"]},\n",
        "    {\"must_mention\" : [\"PyTorch\", \"TensorFlow\"]},\n",
        "    {\"must_mention\" : [\"reduce\", \"parameters\"]},\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfMXF2KAsQxs"
      },
      "source": [
        "####üèóÔ∏è Activity #3:\n",
        "\n",
        "Please create a dataset in the above format with at least 5 questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "CbagRuJop83E"
      },
      "outputs": [],
      "source": [
        "questions = [\n",
        "    \"What is the primary function of the BERT model in NLP?\",\n",
        "    \"How does the Adam optimizer improve training?\",\n",
        "    \"What are the benefits of using dropout in neural networks?\",\n",
        "    \"Which dataset is commonly used for machine translation tasks?\",\n",
        "    \"What is the purpose of normalization layers in deep learning?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"language\", \"understanding\"]},\n",
        "    {\"must_mention\" : [\"adaptive\", \"learning rates\"]},\n",
        "    {\"must_mention\" : [\"prevent\", \"overfitting\"]},\n",
        "    {\"must_mention\" : [\"WMT\", \"dataset\"]},\n",
        "    {\"must_mention\" : [\"standardize\", \"activations\"]}\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7QVFuAmsh7L"
      },
      "source": [
        "Now we can add our dataset to our LangSmith project using the following code which we saw last Thursday!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "RLfrZrgSsn85"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "dataset_name = f\"Retrieval Augmented Generation - Evaluation Dataset - {uuid4().hex[0:8]}\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Questions about NLP and deep learning.\"\n",
        ")\n",
        "\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\" : q} for q in questions],\n",
        "    outputs=answers,\n",
        "    dataset_id=dataset.id,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciV73F9Q04w0"
      },
      "source": [
        "#### ‚ùì Question #3:\n",
        "\n",
        "How are the correct answers associated with the questions?\n",
        "\n",
        "> NOTE: Feel free to indicate if this is problematic or not"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**ANSWER**:\n",
        "- The correct answers are associated with the questions by ensuring the response to each question contains specific keywords mentioned in the answers list. This method helps verify that the responses address the core aspects of each question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lRTXUrTtP9Y"
      },
      "source": [
        "## Task 2: Adding Evaluators\n",
        "\n",
        "Now we can add a custom evaluator to see if our responses contain the expected information.\n",
        "\n",
        "We'll be using a fairly naive exact-match process to determine if our response contains specific strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "QrAUXMFftlAY"
      },
      "outputs": [],
      "source": [
        "from langsmith.evaluation import EvaluationResult, run_evaluator\n",
        "\n",
        "@run_evaluator\n",
        "def must_mention(run, example) -> EvaluationResult:\n",
        "    prediction = run.outputs.get(\"output\") or \"\"\n",
        "    required = example.outputs.get(\"must_mention\") or []\n",
        "    score = all(phrase in prediction for phrase in required)\n",
        "    return EvaluationResult(key=\"must_mention\", score=score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNtHORUh0jZY"
      },
      "source": [
        "#### ‚ùì Question #4:\n",
        "\n",
        "What are some ways you could improve this metric as-is?\n",
        "\n",
        "> NOTE: Alternatively you can suggest where gaps exist in this method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**ANSWER**:\n",
        "- implement semantic keywords check functionality.\n",
        "- add more keywords in must_mention list."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ4DVSXl0BX5"
      },
      "source": [
        "Now that we have created our custom evaluator - let's initialize our `RunEvalConfig` with it, and a few others:\n",
        "\n",
        "- `\"criteria\"` includes the default criteria which, in this case, means \"helpfulness\"\n",
        "- `\"cot_qa\"` includes a criteria that bases whether or not the answer is correct by utilizing a Chain of Thought prompt and the provided context to determine if the response is correct or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "sL4-XcjytWsu"
      },
      "outputs": [],
      "source": [
        "from langchain.smith import RunEvalConfig, run_on_dataset\n",
        "\n",
        "eval_config = RunEvalConfig(\n",
        "    custom_evaluators=[must_mention],\n",
        "    evaluators=[\n",
        "        \"criteria\",\n",
        "        \"cot_qa\",\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1RJr349zhv7"
      },
      "source": [
        "Task 3: Evaluating\n",
        "\n",
        "All that is left to do is evaluate our agent's response!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5TeCUUkuGld",
        "outputId": "045f1296-2bee-43c8-8b92-b24dc242ff88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for project 'RAG Pipeline - Evaluation - 1529d115' at:\n",
            "https://smith.langchain.com/o/4bddf518-e972-53c2-b3a5-9981d8b62077/datasets/b430109c-c9ea-4e70-9f68-57d8eeaba34b/compare?selectedSessions=4e011848-b7dc-4f0f-bc75-58a4bfb8f7d9\n",
            "\n",
            "View all tests for Dataset Retrieval Augmented Generation - Evaluation Dataset - 47716664 at:\n",
            "https://smith.langchain.com/o/4bddf518-e972-53c2-b3a5-9981d8b62077/datasets/b430109c-c9ea-4e70-9f68-57d8eeaba34b\n",
            "[------------------------------------------------->] 5/5"
          ]
        },
        {
          "data": {
            "text/html": [
              "<h3>Experiment Results:</h3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feedback.helpfulness</th>\n",
              "      <th>feedback.COT Contextual Accuracy</th>\n",
              "      <th>feedback.must_mention</th>\n",
              "      <th>error</th>\n",
              "      <th>execution_time</th>\n",
              "      <th>run_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>30682caa-dd6b-4e06-b010-8eee4ae05ea7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.348817</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.375147</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.739464</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.867384</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.445941</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.887007</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6.804289</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        feedback.helpfulness  feedback.COT Contextual Accuracy  \\\n",
              "count                    5.0                               5.0   \n",
              "unique                   NaN                               NaN   \n",
              "top                      NaN                               NaN   \n",
              "freq                     NaN                               NaN   \n",
              "mean                     1.0                               1.0   \n",
              "std                      0.0                               0.0   \n",
              "min                      1.0                               1.0   \n",
              "25%                      1.0                               1.0   \n",
              "50%                      1.0                               1.0   \n",
              "75%                      1.0                               1.0   \n",
              "max                      1.0                               1.0   \n",
              "\n",
              "       feedback.must_mention error  execution_time  \\\n",
              "count                      5     0        5.000000   \n",
              "unique                     2     0             NaN   \n",
              "top                     True   NaN             NaN   \n",
              "freq                       3   NaN             NaN   \n",
              "mean                     NaN   NaN        4.348817   \n",
              "std                      NaN   NaN        2.375147   \n",
              "min                      NaN   NaN        1.739464   \n",
              "25%                      NaN   NaN        1.867384   \n",
              "50%                      NaN   NaN        5.445941   \n",
              "75%                      NaN   NaN        5.887007   \n",
              "max                      NaN   NaN        6.804289   \n",
              "\n",
              "                                      run_id  \n",
              "count                                      5  \n",
              "unique                                     5  \n",
              "top     30682caa-dd6b-4e06-b010-8eee4ae05ea7  \n",
              "freq                                       1  \n",
              "mean                                     NaN  \n",
              "std                                      NaN  \n",
              "min                                      NaN  \n",
              "25%                                      NaN  \n",
              "50%                                      NaN  \n",
              "75%                                      NaN  \n",
              "max                                      NaN  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'project_name': 'RAG Pipeline - Evaluation - 1529d115',\n",
              " 'results': {'1f634075-c854-4e9e-acd0-f6ce12ff0a2e': {'input': {'question': 'What is the primary function of the BERT model in NLP?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides a detailed explanation of the primary function of the BERT model in NLP. It not only defines what BERT is but also explains how it works and what tasks it can perform. The submission also mentions the significance of BERT in improving the performance of various NLP tasks. \\n\\nThis information is helpful for anyone seeking to understand the primary function of the BERT model in NLP. The submission is insightful as it provides a comprehensive understanding of the topic. It is also appropriate as it directly answers the question asked in the input. \\n\\nTherefore, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('3cd77c42-ec80-429f-b744-facf8395839f'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer is in line with the context provided. The BERT model in Natural Language Processing (NLP) is indeed used for understanding the context of words in a sentence by capturing bidirectional relationships between words. The student also correctly mentions that BERT is a pre-trained language model that uses a transformer architecture to learn contextual representations of words in a sentence. The student further explains that BERT can perform tasks such as text classification, named entity recognition, question answering, and more with high accuracy. All these points are factually correct and relevant to the question asked.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('9d637fe2-25f3-4926-b9d3-2a57265beb86'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('9e29cf94-a0ca-4e75-acd3-401732c9e7c1'), target_run_id=None)],\n",
              "   'execution_time': 1.739464,\n",
              "   'run_id': '30682caa-dd6b-4e06-b010-8eee4ae05ea7',\n",
              "   'output': 'The primary function of the BERT (Bidirectional Encoder Representations from Transformers) model in Natural Language Processing (NLP) is to understand the context of words in a sentence by capturing bidirectional relationships between words. BERT is a pre-trained language model that uses a transformer architecture to learn contextual representations of words in a sentence. This allows BERT to perform tasks such as text classification, named entity recognition, question answering, and more with high accuracy. BERT has significantly improved the performance of various NLP tasks by leveraging its ability to understand the context and meaning of words in a sentence.',\n",
              "   'reference': {'must_mention': ['language', 'understanding']}},\n",
              "  '48d19ad4-2b43-4296-9e1f-49e837d9eb96': {'input': {'question': 'How does the Adam optimizer improve training?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\n1. The submission provides a detailed explanation of how the Adam optimizer improves training in machine learning models. It breaks down the concept into five key points, making it easier for the reader to understand.\\n\\n2. Each point is explained clearly and concisely, providing insight into the workings of the Adam optimizer. \\n\\n3. The submission is appropriate for the given input. It directly answers the question and provides relevant information.\\n\\n4. The submission not only explains how the Adam optimizer works but also why these features improve the training process, making it very helpful for someone trying to understand this concept.\\n\\nBased on these points, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('fd58ac7b-31db-4ad2-b78f-70f1e873a6ab'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer is in line with the context provided. The context mentions 'adaptive' and 'learning rates', which are key features of the Adam optimizer as explained by the student. The student goes into detail about how the Adam optimizer uses adaptive learning rates to improve training, and also mentions other features of the optimizer such as momentum, bias correction, regularization, and fast convergence speed. All these points are factually correct and relevant to the question asked. Therefore, the student's answer is correct.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('d8ea1286-e5c6-4cc1-8f50-8b95b4691959'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('bda98c84-7e6b-4820-8847-fd6d9ac1f09c'), target_run_id=None)],\n",
              "   'execution_time': 5.445941,\n",
              "   'run_id': 'f06252cb-11bd-4a91-9e9a-f0bd3243fe9c',\n",
              "   'output': 'The Adam optimizer is an adaptive learning rate optimization algorithm that is designed to improve training in machine learning models. Here are some ways in which the Adam optimizer improves training:\\n\\n1. Adaptive Learning Rate: Adam adjusts the learning rate for each parameter based on the first and second moments of the gradients. This adaptive learning rate helps in converging faster and finding the optimal solution.\\n\\n2. Momentum: Adam uses momentum to accelerate the optimization process by accumulating the exponentially decaying average of past gradients. This helps in smoothing out the optimization process and navigating through local minima.\\n\\n3. Bias Correction: Adam incorporates bias correction to account for the fact that the first and second moments of the gradients are initialized to zero. This correction helps in stabilizing the optimization process and improving convergence.\\n\\n4. Regularization: Adam includes L2 regularization to prevent overfitting and improve the generalization of the model.\\n\\n5. Convergence Speed: Adam is known for its fast convergence speed compared to other optimization algorithms, making it suitable for training deep neural networks and complex models.\\n\\nOverall, the Adam optimizer combines the benefits of adaptive learning rates, momentum, bias correction, and regularization to improve the training process and enhance the performance of machine learning models.',\n",
              "   'reference': {'must_mention': ['adaptive', 'learning rates']}},\n",
              "  '2e18630b-8530-4ea1-8190-33fb14f814ed': {'input': {'question': 'What are the benefits of using dropout in neural networks?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\n1. The submission provides a detailed explanation of what dropout is and how it works in neural networks. This is helpful for someone who is trying to understand the concept.\\n\\n2. The submission lists several benefits of using dropout in neural networks, including reducing overfitting, improving generalization, increasing robustness, faster training, and simplifying model tuning. Each benefit is explained in a clear and understandable manner, which is insightful for the reader.\\n\\n3. The submission is appropriate for the given input. It directly answers the question and provides relevant information.\\n\\nBased on these points, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('1105e108-24de-494d-b573-3c0974196f69'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer is in line with the context provided. The context mentions 'prevent' and 'overfitting', which are key benefits of using dropout in neural networks. The student's answer elaborates on these points and provides additional benefits of using dropout in neural networks. The student's answer does not contradict the context and provides a comprehensive explanation of the benefits of using dropout in neural networks.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('9dea35f2-0658-49b0-b31c-fff0b0401b10'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('704d1aeb-160a-4ec3-aa2d-7c7d0c488ee9'), target_run_id=None)],\n",
              "   'execution_time': 5.887007,\n",
              "   'run_id': '84d7573b-1f1c-4966-9494-1a105ffabe32',\n",
              "   'output': 'Dropout is a regularization technique used in neural networks to prevent overfitting and improve the generalization of the model. Here are some benefits of using dropout in neural networks:\\n\\n1. **Reduces Overfitting**: Dropout helps in reducing overfitting by randomly dropping out (setting to zero) a proportion of the neurons during training. This prevents the network from relying too much on specific neurons and forces it to learn more robust features.\\n\\n2. **Improves Generalization**: By preventing co-adaptation of neurons, dropout encourages each neuron to learn more independent and useful features. This leads to better generalization on unseen data.\\n\\n3. **Increases Robustness**: Dropout makes the network more robust to noise and variations in the input data. It helps in creating a more stable and reliable model that performs well on different datasets.\\n\\n4. **Faster Training**: Dropout can act as a form of ensemble learning by training multiple subnetworks with shared parameters. This can lead to faster convergence during training and better performance.\\n\\n5. **Simplifies Model Tuning**: Dropout provides a simple and effective way to regularize neural networks without the need for extensive hyperparameter tuning. It can be easily incorporated into the network architecture.\\n\\nOverall, dropout is a powerful technique that can improve the performance and robustness of neural networks, especially in scenarios where overfitting is a concern.',\n",
              "   'reference': {'must_mention': ['prevent', 'overfitting']}},\n",
              "  '8a6401ec-04ef-4f6b-aacb-49b42f6038eb': {'input': {'question': 'Which dataset is commonly used for machine translation tasks?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides a clear and detailed answer to the input question. It not only names a commonly used dataset for machine translation tasks (WMT), but also explains what the dataset is and how it is used. This additional information makes the answer more helpful and insightful for someone who may not be familiar with the WMT dataset. \\n\\nThe submission is also appropriate as it directly addresses the input question and stays on topic. It does not include any irrelevant or inappropriate content. \\n\\nBased on this analysis, the submission meets the criterion of being helpful, insightful, and appropriate. \\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('edead3c5-2638-4951-9009-8719e973417b'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer correctly identifies the WMT dataset as a commonly used dataset for machine translation tasks. The additional information provided about the WMT dataset is accurate and does not conflict with the context. Therefore, the student's answer is correct.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('56ce0e3c-4b44-48b6-b7d6-1c5f936cf61f'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('563b4b7e-84ff-4323-8bea-675c60e11f32'), target_run_id=None)],\n",
              "   'execution_time': 1.867384,\n",
              "   'run_id': '9e610779-c132-4a09-acc3-fcc70a14e4b0',\n",
              "   'output': 'One commonly used dataset for machine translation tasks is the WMT (Workshop on Machine Translation) dataset. The WMT dataset is a collection of parallel corpora in multiple languages that are used for training and evaluating machine translation models. It includes translations of text in various domains and is widely used in research and development of machine translation systems.',\n",
              "   'reference': {'must_mention': ['WMT', 'dataset']}},\n",
              "  '02caff28-6659-4544-9b93-cd0df8fd160b': {'input': {'question': 'What is the purpose of normalization layers in deep learning?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\n1. The submission provides a detailed explanation of the purpose of normalization layers in deep learning. It not only defines what normalization layers are but also explains their key purposes in a clear and understandable manner.\\n\\n2. The submission is insightful as it goes beyond a basic explanation and discusses the role of normalization layers in reducing internal covariate shift, stabilizing gradient flow, improving generalization, and enabling faster training. \\n\\n3. The submission is appropriate as it directly answers the question asked in the input and provides relevant information. It does not include any irrelevant or inappropriate content.\\n\\nBased on these points, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('9e2a9c4b-a5bb-4169-97d7-124fa6d431a8'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer is comprehensive and covers the main purposes of normalization layers in deep learning. The student correctly identifies that normalization layers are used to standardize activations, reduce internal covariate shift, stabilize gradient flow, improve generalization, and enable faster training. All these points are factually correct and relevant to the context provided. Therefore, the student's answer is correct.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('8f20040f-9384-4939-9d5c-71973b6a641a'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('8c03c8f3-90a5-4240-966d-0e92b3aa0d51'), target_run_id=None)],\n",
              "   'execution_time': 6.804289,\n",
              "   'run_id': 'a5eda00b-91e0-4b30-b0cd-c691387d78d0',\n",
              "   'output': 'Normalization layers in deep learning are used to improve the training of neural networks by addressing issues related to internal covariate shift and gradient vanishing/exploding. Here are some key purposes of normalization layers in deep learning:\\n\\n1. **Internal Covariate Shift**: Normalization layers help in reducing internal covariate shift, which is the change in the distribution of network activations due to changes in the network parameters during training. By normalizing the inputs to each layer, normalization layers help in stabilizing the training process and speeding up convergence.\\n\\n2. **Stabilizing Gradient Flow**: Normalization layers help in stabilizing the flow of gradients during backpropagation by ensuring that the gradients are within a reasonable range. This helps in preventing issues such as vanishing or exploding gradients, which can hinder the training of deep neural networks.\\n\\n3. **Improving Generalization**: Normalization layers can also help in improving the generalization of the model by reducing overfitting. By normalizing the inputs to each layer, normalization layers prevent the model from relying too much on specific features and help in learning more robust and generalizable representations.\\n\\n4. **Enabling Faster Training**: Normalization layers can enable faster training of deep neural networks by allowing for higher learning rates and more stable training dynamics. This can lead to faster convergence and better performance on the training data.\\n\\nOverall, normalization layers play a crucial role in improving the stability, convergence speed, and generalization of deep neural networks, making them an essential component in modern deep learning architectures.',\n",
              "   'reference': {'must_mention': ['standardize', 'activations']}}},\n",
              " 'aggregate_metrics': None}"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "client.run_on_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=agent_chain,\n",
        "    evaluation=eval_config,\n",
        "    verbose=True,\n",
        "    project_name=f\"RAG Pipeline - Evaluation - {uuid4().hex[0:8]}\",\n",
        "    project_metadata={\"version\": \"1.0.0\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1wKRddbIY_S"
      },
      "source": [
        "## Part 2:\n",
        "\n",
        "Now that we've done evaluation - let's see if we can add an extra step where we review the content we've generated to confirm if it fully answers the user's query!\n",
        "\n",
        "We're going to make a few key adjustments to account for this:\n",
        "\n",
        "1. We're going to add an artificial limit on how many \"loops\" the agent can go through - this will help us to avoid the potential situation where we never exit the loop.\n",
        "2. We'll add a custom node and conditional edge to determine if the response was helpful enough."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npTYJ8ayR5B3"
      },
      "source": [
        "First, let's define our state again - we can check the length of the state object, so we don't need additional state for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "-LQ84YhyJG0w"
      },
      "outputs": [],
      "source": [
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gC8t-4FISCEh"
      },
      "source": [
        "We're going to add a custom helpfulness check here!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "ZV_PxI5zNY7f"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def check_helpfulness(state):\n",
        "  initial_query = state[\"messages\"][0]\n",
        "  final_response = state[\"messages\"][-1]\n",
        "\n",
        "  if len(state[\"messages\"]) > 10:\n",
        "    return \"END\"\n",
        "\n",
        "  prompt_template = \"\"\"\\\n",
        "  Given an initial query and a final response, determine if the final response is extremely helpful or not. Please indicate helpfulness with a 'Y' and unhelpfulness as an 'N'.\n",
        "\n",
        "  Initial Query:\n",
        "  {initial_query}\n",
        "\n",
        "  Final Response:\n",
        "  {final_response}\"\"\"\n",
        "\n",
        "  prompt_template = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "  helpfulness_check_model = ChatOpenAI(model=\"gpt-4\")\n",
        "\n",
        "  helpfulness_chain = prompt_template | helpfulness_check_model | StrOutputParser()\n",
        "\n",
        "  helpfulness_response = helpfulness_chain.invoke({\"initial_query\" : initial_query.content, \"final_response\" : final_response.content})\n",
        "\n",
        "  if \"Y\" in helpfulness_response:\n",
        "    print(\"Helpful!\")\n",
        "    return \"end\"\n",
        "  else:\n",
        "    print(\"Not helpful!\")\n",
        "    return \"continue\"\n",
        "\n",
        "def dummy_node(state):\n",
        "  return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz1u9Vf4SHxJ"
      },
      "source": [
        "####üèóÔ∏è Activity #3:\n",
        "\n",
        "Please write what is happening in our `check_helpfulness` function!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**ANSWER**:\n",
        "- The function check_helpfulness in the given Python code evaluates the helpfulness of responses in a conversational state. It checks if the number of messages exceeds 10, and if so, it concludes the process. If not, it creates a prompt to determine whether the final response adequately addressed the initial query. The prompt asks for a judgment of 'extremely helpful' or 'not', marked by 'Y' for yes and 'N' for no. It uses a PromptTemplate and a ChatOpenAI model for generating the helpfulness check and parses the output with StrOutputParser. Based on the model's output, it prints if the response was helpful or not and dictates whether to continue or end the interaction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD7EV0HqSQcb"
      },
      "source": [
        "Now we can set our graph up! This process will be almost entirely the same - with the inclusion of one additional node/conditional edge!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "6r6XXA5FJbVf"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check = StateGraph(AgentState)\n",
        "\n",
        "graph_with_helpfulness_check.add_node(\"agent\", call_model)\n",
        "graph_with_helpfulness_check.add_node(\"action\", call_tool)\n",
        "graph_with_helpfulness_check.add_node(\"passthrough\", dummy_node)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "HNWHwWxuRiLY"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "aVTKnWMbP_8T"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\" : \"action\",\n",
        "        \"end\" : \"passthrough\"\n",
        "    }\n",
        ")\n",
        "\n",
        "graph_with_helpfulness_check.add_conditional_edges(\n",
        "    \"passthrough\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\" : \"agent\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "cbDK2MbuREgU"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OWOPweuSiHc"
      },
      "source": [
        "Let's compile and test!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "oQldl8ERQ8lf"
      },
      "outputs": [],
      "source": [
        "agent_with_helpfulness_check = graph_with_helpfulness_check.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3oo8E-PRK1T",
        "outputId": "7e2ea696-ba42-468e-a842-2c585f246c12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Query: Related to machine learning, what is LoRA? Also, who is Tim Dettmers? Also, what is Attention?\n",
            "\n",
            "\n",
            "Tool Call - Name: duckduckgo_search + Query: {\"query\":\"LoRA machine learning\"}\n",
            "Tool Response: \"Lora The Tuner\" By Daniel Warfield using MidJourney. All images by the author unless otherwise specified. Fine tuning is the process of tailoring a machine learning model to a specific application, which can be vital in achieving consistent and high quality performance. Let's jump on LoRA. Low-Rank Adaptation of LLMs (LoRA) So, in usual fine-tuning, we. Take a pretrained model. Do Transfer Learning over new training data to slightly adjust these pre-trained weights Feb 18, 2024. Low-rank adaptation (LoRA) is a machine learning technique that modifies a pretrained model (for example, an LLM or vision transformer) to better suit a specific, often smaller, dataset by adjusting only a small, low-rank subset of the model's parameters. This approach is important because it allows for efficient finetuning of ... And that is the aim of LoRA ‚Äî to take the change-in-weight matrix and approximate it with the product of two lower-rank matrices. ... Machine Learning. Lora----1. Follow. Written by AR. 28 ... In the evolving landscape of machine learning and natural language processing, two distinct methodologies have emerged, each with its‚Ä¶ ¬∑ 2 min read ¬∑ Jan 27, 2024\n",
            "\n",
            "Tool Call - Name: duckduckgo_search + Query: {\"query\":\"Tim Dettmers\"}\n",
            "Tool Response: QLoRA: Efficient Finetuning of Quantized LLMs. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer. We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through ... Sponsored by Evolution AI: https://www.evolution.aiAbstract: Recent open-source large language models (LLMs) like LLaMA and Falcon are both high-quality and ... Allen School Ph.D. student Tim Dettmers accepted the grand prize for QLoRA, a novel approach to finetuning pretrained models that significantly reduces the amount of GPU memory required ‚Äî from over 780GB to less than 48GB ‚Äî to finetune a 65B parameter model. With QLoRA, the largest publicly available models can be finetuned on a single ... View a PDF of the paper titled SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression, by Tim Dettmers and 8 other authors. View PDF Abstract: Recent advances in large language model (LLM) pretraining have led to high-quality LLMs with impressive abilities. By compressing such LLMs via quantization to 3-4 bits per ... Its purpose is to make cutting-edge research by Tim Dettmers, a leading academic expert on quantization and the use of deep learning hardware accelerators, accessible to the general public. QLoRA: One of the core contributions of bitsandbytes towards the democratization of AI.\n",
            "\n",
            "Tool Call - Name: duckduckgo_search + Query: {\"query\":\"Attention in machine learning\"}\n",
            "Tool Response: The attention mechanism is a technique used in machine learning and natural language processing to increase model accuracy by focusing on relevant data. It enables the model to focus on certain areas of the input data, giving more weight to crucial features and disregarding unimportant ones. There are several types of attention mechanisms, each designed to cater to specific use cases. Here are a few notable ones: 1. Self-Attention Mechanism. Self-attention, also known as intra ... Neural Machine Translation by Jointly Learning to Align and Translate. [3] Luong, M. T., et al. (2015). Effective Approaches to Attention-based Neural Machine Translation. About the Author. Dr. Barak Or is a professional in the field of artificial intelligence and sensor fusion. He is a researcher, lecturer, and entrepreneur who has published ... The concept of \"attention\" in deep learning has its roots in the effort to improve Recurrent Neural Networks (RNNs) for handling longer sequences or sentences. For instance, consider translating a sentence from one language to another. Translating a sentence word-by-word is usually not an option because it ignores the complex grammatical ... The attention mechanism was originally popularized in Neural Machine Translation by Jointly Learning to Align and Translate(2014), which is the guiding reference for this particular post. This paper employs an encoder-decoder architecture for english-to-french translation.\n",
            "\n",
            "Agent Response: - LoRA stands for Low-Rank Adaptation of LLMs. It is a machine learning technique that modifies a pretrained model to better suit a specific dataset by adjusting only a small, low-rank subset of the model's parameters. This approach allows for efficient fine-tuning of models.\n",
            "\n",
            "- Tim Dettmers is a researcher and expert in the field of artificial intelligence and machine learning. He is known for his work on efficient fine-tuning approaches for pretrained models, such as QLoRA, which reduces memory usage for fine-tuning large language models on GPUs.\n",
            "\n",
            "- Attention is a technique used in machine learning and natural language processing to improve model accuracy by focusing on relevant data. It allows the model to give more weight to important features and ignore unimportant ones. There are different types of attention mechanisms designed for specific use cases, such as self-attention mechanisms.\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"Related to machine learning, what is LoRA? Also, who is Tim Dettmers? Also, what is Attention?\")]}\n",
        "\n",
        "messages = agent_with_helpfulness_check.invoke(inputs)\n",
        "\n",
        "print_messages(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Query: What is arxiv?\n",
            "\n",
            "\n",
            "Agent Response: ArXiv is a repository of electronic preprints (known as e-prints) approved for publication after moderation, allowing researchers to share their work before it is formally peer-reviewed and published in academic journals. ArXiv covers a wide range of disciplines, including Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics. Researchers can upload their papers to ArXiv to share their findings with the scientific community and receive feedback before formal publication.\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"What is arxiv?\")]}\n",
        "\n",
        "messages = agent_with_helpfulness_check.invoke(inputs)\n",
        "\n",
        "print_messages(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
