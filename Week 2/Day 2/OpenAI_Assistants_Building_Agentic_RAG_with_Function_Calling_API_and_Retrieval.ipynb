{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgDepNVhvzIr"
      },
      "source": [
        "# OpenAI Assistants - Building Agentic RAG with the Function Calling, Retrieval, and Code Interpreter Tools\n",
        "\n",
        "Today we'll explore using OpenAI's Python SDK to create, manage, and use the OpenAI Assistant API!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNU6b3ymwOWq"
      },
      "source": [
        "## Dependencies\n",
        "\n",
        "We'll start, as we usually do, with some dependiencies and our API key!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePayyL6at6LS"
      },
      "outputs": [],
      "source": [
        "!pip install -qU openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unKr3HZdu-1V",
        "outputId": "610aeddb-5e56-4d75-e399-59054435c2ef"
      },
      "outputs": [],
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwNE7N4HwhXH"
      },
      "source": [
        "## Simple Assistant\n",
        "\n",
        "Let's create a simple Assistant to understand more about how the API works to start!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKEwbLMFxNKs"
      },
      "source": [
        "### OpenAI Client\n",
        "\n",
        "At the core of the OpenAI Python SDK is the Client!\n",
        "\n",
        "> NOTE: For ease of use, we'll start with the synchronous `OpenAI()`. OpenAI does provide an `AsyncOpenAI()` that you could leverage as well!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wF-mBZwtuavl"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aIx4GZ2w_1c"
      },
      "source": [
        "### Creating An Assistant\n",
        "\n",
        "Leveraging what we know about the OpenAI API from previous sessions - we're going to start by simply initializing an Assistant.\n",
        "\n",
        "Before we begin, we need to think about a few customization options we have:\n",
        "\n",
        "- `name` - Straight forward enough, this is what our Assistant's name will be\n",
        "- `instructions` - similar to a system message, but applied at an Assistant level, this is how we can guide the Assistant's tone, behaviour, functionality, and more!\n",
        "- `model` - this will allow us to choose which model we would prefer to use for our Assistant\n",
        "\n",
        "Let's start by setting some instructions for our Assistant.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "Paqd6zWMyMAJ"
      },
      "outputs": [],
      "source": [
        "# @markdown #### 🏗️ Build Activity 🏗️\n",
        "# @markdown Fill out the fields below to add your Assistant's name, instructions, and desired model!\n",
        "\n",
        "name = \"ML & DL Assistant\" # @param {type: \"string\"}\n",
        "instructions = '''You are an expert assistant answering technical questions on machine learning and deep learning subject.\n",
        "Ensure that your response is unbiased and generic, you will be 'AWARDED' for giving really good clarity and correct answers.''' # @param {type: \"string\"}\n",
        "model = \"gpt-3.5-turbo\" # @param [\"gpt-3.5-turbo\", \"gpt-4-turbo-preview\", \"gpt-4\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUeaDsLMzcv-"
      },
      "source": [
        "### Initialize Assistant\n",
        "\n",
        "Now that we have our desired name, instruction, and model - we can initialize our Assistant!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6-4MgVLbu8rO"
      },
      "outputs": [],
      "source": [
        "assistant = client.beta.assistants.create(\n",
        "    name=name,\n",
        "    instructions=instructions,\n",
        "    model=model,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QskO5n5W2X6t"
      },
      "source": [
        "Let's examine our `assistant` object and see what we find!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkkIC_JP2bG0",
        "outputId": "a1646ea4-dc68-4a3d-f997-c3e7f0fd7abd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Assistant(id='asst_MapOlLsMbKqSNbRqi2fzFWMK', created_at=1713061548, description=None, file_ids=[], instructions=\"You are an expert assistant answering technical questions on machine learning and deep learning subject.\\nEnsure that your response is unbiased and generic, you will be 'AWARDED' for giving really good clarity and correct answers.\", metadata={}, model='gpt-3.5-turbo', name='ML & DL Assistant', object='assistant', tools=[])\n"
          ]
        }
      ],
      "source": [
        "print(assistant)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "615NK1Qj2e_Z"
      },
      "source": [
        "There are a number of useful parameters here, but we'll call out a few:\n",
        "\n",
        "- `id` - since we may have multiple Assistant's, knowing which Assistant we're interacting with will help us ensure the desired user experience!\n",
        "- `description` - A natrual language description of our Assistant could help others understand what it's supposed to do!\n",
        "- `file_ids` - if we wanted to use the Retrieval tool, this would let us know what files we had given our Assistant"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3HhlqtM0AhW"
      },
      "source": [
        "### Creating a Thread\n",
        "\n",
        "Behind the scenes our Assistant is powered by the idea of \"threads\".\n",
        "\n",
        "You can think of threads as individual conversations that interact with the Assistant.\n",
        "\n",
        "Let's create a thread now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iFVM39vevT5f"
      },
      "outputs": [],
      "source": [
        "thread = client.beta.threads.create()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y7jelq01PoG"
      },
      "source": [
        "Let's look at our `thread` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5V8WAKDZ1Uf2",
        "outputId": "4678aaef-3c01-4876-8136-334897074ad7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Thread(id='thread_i0fXOv17V711RAdnpxdBL4W7', created_at=1713061554, metadata={}, object='thread')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "thread"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4k6S_e501V4z"
      },
      "source": [
        "Notice some key attributes:\n",
        "\n",
        "- `id` - since each Thread is like a conversation, we need some way to specify which thread we're dealing with when interacting with them\n",
        "- `tool_resources` - this will become more relevant as we add tools since we'll need a way to verify which tools we have access to when interacting with our Assistant"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5BvGv1N0c2h"
      },
      "source": [
        "### Adding Messages to Our Thread\n",
        "\n",
        "Now that we have our Thread (or conversation) we can start adding messages to it!\n",
        "\n",
        "Let's add a simple message that asks about how our Assistant is feeling.\n",
        "\n",
        "Notice the parameters we're leveraging:\n",
        "\n",
        "- `thread_id` - since each Thread is like a conversation, we need some way to address a specific conversation. We can use `thread.id` to do this.\n",
        "- `role` - similar to when we used our chat completions endpoint, this parameter specifies who the message is coming from. You can leverage this in the same ways you would through the chat completions endpoint.\n",
        "- `content` - this is where we can place the actual text our Assistant will interact with\n",
        "\n",
        "> NOTE: Feel free to substitute a relevant message based on the Assistant you created"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'thread_i0fXOv17V711RAdnpxdBL4W7'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "thread.id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "R7ZNCfGivagg"
      },
      "outputs": [],
      "source": [
        "message = client.beta.threads.messages.create(\n",
        "    thread_id=thread.id,\n",
        "    role=\"user\",\n",
        "    content=f\"What is Deep Learning - DL?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc7R3Sr32P0b"
      },
      "source": [
        "Again, let's examine our `message` object!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMLvyZDA2S_D",
        "outputId": "5076c8e4-0970-44ff-c609-1501a53cfe1c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Message(id='msg_MmxTeK7WTHulI0hvPonbnnAA', assistant_id=None, completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value='What is Deep Learning - DL?'), type='text')], created_at=1713061560, file_ids=[], incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='user', run_id=None, status=None, thread_id='thread_i0fXOv17V711RAdnpxdBL4W7')"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "message"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI3Pctpk29og"
      },
      "source": [
        "### Running Our Thread\n",
        "\n",
        "Now that we have an Assistant, and we've given that Assistant a Thread, and we've added a Message to that Thread - we're ready to run our Assistant!\n",
        "\n",
        "Notice that this process lets us add (potentially) multiple messages to our Assistant. We can leverage that behaviour for few/many-shot examples, and more!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "cellView": "form",
        "id": "VkvsXv5_3cyQ"
      },
      "outputs": [],
      "source": [
        "# @markdown #### 🏗️ Build Activity 🏗️\n",
        "# @markdown We can also override the Assistant's instructions when we run a thread.\n",
        "\n",
        "# @markdown Use one of the [Prompt Principles for Instruction](https://arxiv.org/pdf/2312.16171v1.pdf) to improve the likeliehood of a correct or valuable response from your Assistant.\n",
        "\n",
        "additional_instructions = '''Think and give only explanation or code or links for resources or steps for project, for the questions asked along with response.\n",
        "##EXAMPLES##\n",
        "If users asks 'explain neural networks', your response should be with an overview of neural networks, discussing how they are computational models inspired by the human brain that are used to recognize patterns and solve complex problems in machine learning.\n",
        "If users ask 'code convolutional neural network', your response should contain example of the code necessary to create a CNN.\n",
        "If users ask 'resource gradient descent', your response should offer links to tutorials, video lectures, or articles that explain gradient descent, which is an optimization algorithm used to minimize a function by iteratively moving in the direction of steepest descent.\n",
        "If users ask 'project sentiment analysis', your response should discuss the steps involved in creating a sentiment analysis model, such as data collection, preprocessing, model selection, training, and evaluation, and potentially offer advice on best practices or methodologies to consider.\n",
        "''' # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CuGbTrL5QEc"
      },
      "source": [
        "Let's run our Thread!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "fpWNl3UVvdW4"
      },
      "outputs": [],
      "source": [
        "run = client.beta.threads.runs.create(\n",
        "  thread_id=thread.id,\n",
        "  assistant_id=assistant.id,\n",
        "  instructions=instructions + \" \" + additional_instructions\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erMGdU7y6la2"
      },
      "source": [
        "Now that we've run our thread, let's look at the object!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "asst_MapOlLsMbKqSNbRqi2fzFWMK\n",
            "thread_i0fXOv17V711RAdnpxdBL4W7\n",
            "msg_MmxTeK7WTHulI0hvPonbnnAA\n"
          ]
        }
      ],
      "source": [
        "print(assistant.id)\n",
        "print(thread.id)\n",
        "print(message.id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kz_rfwi869YI",
        "outputId": "9b0df335-c3a9-43a5-f7c0-020e37679c8b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Run(id='run_4G5mZzo13ma3r5aQA9yRmM7F', assistant_id='asst_MapOlLsMbKqSNbRqi2fzFWMK', cancelled_at=None, completed_at=None, created_at=1713061683, expires_at=1713062283, failed_at=None, file_ids=[], instructions=\"You are an expert assistant answering technical questions on machine learning and deep learning subject.\\nEnsure that your response is unbiased and generic, you will be 'AWARDED' for giving really good clarity and correct answers. Think and give only explanation or code or links for resources or steps for project, for the questions asked along with response.\\n##EXAMPLES##\\nIf users asks 'explain neural networks', your response should be with an overview of neural networks, discussing how they are computational models inspired by the human brain that are used to recognize patterns and solve complex problems in machine learning.\\nIf users ask 'code convolutional neural network', your response should contain example of the code necessary to create a CNN.\\nIf users ask 'resource gradient descent', your response should offer links to tutorials, video lectures, or articles that explain gradient descent, which is an optimization algorithm used to minimize a function by iteratively moving in the direction of steepest descent.\\nIf users ask 'project sentiment analysis', your response should discuss the steps involved in creating a sentiment analysis model, such as data collection, preprocessing, model selection, training, and evaluation, and potentially offer advice on best practices or methodologies to consider.\\n\", last_error=None, metadata={}, model='gpt-3.5-turbo', object='thread.run', required_action=None, started_at=None, status='queued', thread_id='thread_i0fXOv17V711RAdnpxdBL4W7', tools=[], usage=None, temperature=1.0, max_completion_tokens=None, max_prompt_tokens=None, truncation_strategy={'type': 'auto', 'last_messages': None}, incomplete_details=None, response_format='auto', tool_choice='auto')"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h2SH_347JJb"
      },
      "source": [
        "Notice we have access to a few very powerful parameters in this `run` object.\n",
        "\n",
        "- `completed_at` - this will help us determine when we can expect to retrieve a response\n",
        "- `failed_at` - this can highlight any issues our run ran into\n",
        "- `status` - is another way we can understand how the flow is going"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVBNagBU7kpx"
      },
      "source": [
        "### Retrieving Our Run\n",
        "\n",
        "Now that we've created our run, let's retrieve it.\n",
        "\n",
        "We're going to wrap this in a simple loop to make sure we're not retrieving it too early."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'completed'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "run.status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "itz5_otPvfkV"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "while run.status == \"in_progress\" or run.status == \"queued\":\n",
        "  time.sleep(1)\n",
        "  run = client.beta.threads.runs.retrieve(\n",
        "    thread_id=thread.id,\n",
        "    run_id=run.id\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgGE1uUJ7z3h",
        "outputId": "174572f2-fcb6-4b5e-9ad0-6a6b9c413f79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "completed\n"
          ]
        }
      ],
      "source": [
        "print(run.status)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHVTS4hD7-fv"
      },
      "source": [
        "Now that our run is completed - we can retieve the messages from our thread!\n",
        "\n",
        "Notice that our run helps us understand how things are going - but it isn't where we're going to find our responses or messages. Those are added on the backend into our thread.\n",
        "\n",
        "This leads to a simple, but important, flow:\n",
        "\n",
        "1. We add messages to a thread.\n",
        "2. We create a run on that thread.\n",
        "3. We wait until the run is finished.\n",
        "4. We check our thread for the new messages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGBNpGmh-ZpW"
      },
      "source": [
        "### Checking Our Thread\n",
        "\n",
        "Now we can get a list of messages from our thread!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "asst_MapOlLsMbKqSNbRqi2fzFWMK\n",
            "thread_i0fXOv17V711RAdnpxdBL4W7\n",
            "msg_MmxTeK7WTHulI0hvPonbnnAA\n",
            "run_4G5mZzo13ma3r5aQA9yRmM7F\n"
          ]
        }
      ],
      "source": [
        "print(assistant.id)\n",
        "print(thread.id)\n",
        "print(message.id)\n",
        "print(run.id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Av-OQDUPvhAd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SyncCursorPage[Message](data=[Message(id='msg_Ky5c8n1HWWgKES8y4sT8g1Rp', assistant_id='asst_MapOlLsMbKqSNbRqi2fzFWMK', completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value=\"Deep Learning (DL) is a subfield of machine learning that focuses on teaching computers to learn and make decisions by simulating the human brain's neural networks. It involves the use of deep neural networks, which are composed of multiple layers of interconnected nodes (neurons) that work together to process data and extract relevant features. DL is particularly suited for handling complex tasks such as image and speech recognition, natural language processing, and other types of pattern recognition applications. By learning from large amounts of labeled data, deep learning models can automatically discover intricate patterns and relationships in the data, leading to highly accurate predictions and insights.\"), type='text')], created_at=1713061683, file_ids=[], incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='assistant', run_id='run_4G5mZzo13ma3r5aQA9yRmM7F', status=None, thread_id='thread_i0fXOv17V711RAdnpxdBL4W7'), Message(id='msg_MmxTeK7WTHulI0hvPonbnnAA', assistant_id=None, completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value='What is Deep Learning - DL?'), type='text')], created_at=1713061560, file_ids=[], incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='user', run_id=None, status=None, thread_id='thread_i0fXOv17V711RAdnpxdBL4W7')], object='list', first_id='msg_Ky5c8n1HWWgKES8y4sT8g1Rp', last_id='msg_MmxTeK7WTHulI0hvPonbnnAA', has_more=False)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "messages = client.beta.threads.messages.list(\n",
        "  thread_id=thread.id\n",
        ")\n",
        "\n",
        "messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GM6cZk-GviqX",
        "outputId": "03f1a135-ce62-42e4-9e5e-203c91ffaf83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Message(id='msg_Ky5c8n1HWWgKES8y4sT8g1Rp', assistant_id='asst_MapOlLsMbKqSNbRqi2fzFWMK', completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value=\"Deep Learning (DL) is a subfield of machine learning that focuses on teaching computers to learn and make decisions by simulating the human brain's neural networks. It involves the use of deep neural networks, which are composed of multiple layers of interconnected nodes (neurons) that work together to process data and extract relevant features. DL is particularly suited for handling complex tasks such as image and speech recognition, natural language processing, and other types of pattern recognition applications. By learning from large amounts of labeled data, deep learning models can automatically discover intricate patterns and relationships in the data, leading to highly accurate predictions and insights.\"), type='text')], created_at=1713061683, file_ids=[], incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='assistant', run_id='run_4G5mZzo13ma3r5aQA9yRmM7F', status=None, thread_id='thread_i0fXOv17V711RAdnpxdBL4W7'), Message(id='msg_MmxTeK7WTHulI0hvPonbnnAA', assistant_id=None, completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value='What is Deep Learning - DL?'), type='text')], created_at=1713061560, file_ids=[], incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='user', run_id=None, status=None, thread_id='thread_i0fXOv17V711RAdnpxdBL4W7')]\n",
            "2\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Message(id='msg_MmxTeK7WTHulI0hvPonbnnAA', assistant_id=None, completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value='What is Deep Learning - DL?'), type='text')], created_at=1713061560, file_ids=[], incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='user', run_id=None, status=None, thread_id='thread_i0fXOv17V711RAdnpxdBL4W7')"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(messages.data)\n",
        "print(len(messages.data))\n",
        "messages.data[-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgnY16tjCmc6"
      },
      "source": [
        "## Adding Tools\n",
        "\n",
        "Now that we have an understanding of how Assistant works, we can start thinking about adding tools.\n",
        "\n",
        "We'll go through 3 separate tools and explore how we can leverage them!\n",
        "\n",
        "Let's start with the most familiar tool - the Retriever!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0NagnlZC8g9"
      },
      "source": [
        "### Creating an Assistant with the Retriever Tool\n",
        "\n",
        "The first thing we'll want to do is create an assistant with the Retriever tool.\n",
        "\n",
        "This is also going to require some data. We'll provided data - but you're very much encouraged to use your own files to explore how the Assistant works for your use case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HInYwNiQEjQH"
      },
      "source": [
        "#### Collect and Add Data\n",
        "\n",
        "First, we need some data. Second, we need to add the data to our Assistant!\n",
        "\n",
        "Let's start with grabbing some data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "wvAHBszIEa1Y"
      },
      "outputs": [],
      "source": [
        "!wget https://www.gutenberg.org/files/84/84-h/84-h.htm -o frankenstein.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2EpY1w_FQ3m"
      },
      "source": [
        "Now we can upload our file!\n",
        "\n",
        "Pay attention to [this](https://platform.openai.com/docs/assistants/tools/supported-files) documentation to see what kinds of files can be uploaded.\n",
        "\n",
        "> NOTE: Per the OpenAI [docs](https://platform.openai.com/docs/assistants/tools/knowledge-retrieval) The maximum file size is 512 MB and no more than 2,000,000 tokens (computed automatically when you attach a file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "dpVoe2SMFI6s"
      },
      "outputs": [],
      "source": [
        "file_reference = client.files.create(\n",
        "  file=open(\"frankenstein.html\", \"rb\"),\n",
        "  purpose='assistants'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "FileObject(id='file-U9s7r4kC9qiY6PfAGrtsZRWw', bytes=1218, created_at=1713069003, filename='frankenstein.html', object='file', purpose='assistants', status='processed', status_details=None)"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "file_reference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_reference = client.files.create(\n",
        "  file=open('/Users/nithin.kamavaram/Desktop/learning/AI_MAKERSPACE/Resources/week2/session4/2104.05314.pdf', \"rb\"),\n",
        "  purpose='assistants'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "FileObject(id='file-RiqjOiU8HHACTiJMyYFfWkrU', bytes=406660, created_at=1713069007, filename='2104.05314.pdf', object='file', purpose='assistants', status='processed', status_details=None)"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "file_reference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBQOSGyyF2u5"
      },
      "source": [
        "Let's look at what our `file_reference` contains!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJrVLkMpFgwf",
        "outputId": "b8e8d01a-faf8-4308-cc49-54c11415e245"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "FileObject(id='file-RiqjOiU8HHACTiJMyYFfWkrU', bytes=406660, created_at=1713069007, filename='2104.05314.pdf', object='file', purpose='assistants', status='processed', status_details=None)"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "file_reference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[FileObject(id='file-RiqjOiU8HHACTiJMyYFfWkrU', bytes=406660, created_at=1713069007, filename='2104.05314.pdf', object='file', purpose='assistants', status='processed', status_details=None),\n",
              " FileObject(id='file-U9s7r4kC9qiY6PfAGrtsZRWw', bytes=1218, created_at=1713069003, filename='frankenstein.html', object='file', purpose='assistants', status='processed', status_details=None),\n",
              " FileObject(id='file-WhUMv6t7gcHdMDYbxmTNp0Zv', bytes=406660, created_at=1713068962, filename='2104.05314.pdf', object='file', purpose='assistants', status='processed', status_details=None),\n",
              " FileObject(id='file-mPKBxGbGskWHA58taJ3QMvXi', bytes=1218, created_at=1713068656, filename='frankenstein.html', object='file', purpose='assistants', status='processed', status_details=None)]"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "client.files.list().data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jd4O4dpZF-eH"
      },
      "source": [
        "#### Create and Use Assistant\n",
        "\n",
        "Now that we have our file - we can attach it to an Assistant, and we can give that Assistant the ability to use it for retrieval through the Retrieval tool!\n",
        "\n",
        "> NOTE: Please pay attention to [pricing](https://platform.openai.com/docs/assistants/tools/knowledge-retrieval) and don't forget to delete your files when you're done!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'file-RiqjOiU8HHACTiJMyYFfWkrU'"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "file_reference.id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "jfn_MlJqFiEe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Assistant(id='asst_3FIxHxogEnep6NAnJw2vbukr', created_at=1713104249, description=None, file_ids=['file-RiqjOiU8HHACTiJMyYFfWkrU'], instructions=\"You are an expert assistant answering technical questions on machine learning and deep learning subject.\\nEnsure that your response is unbiased and generic, you will be 'AWARDED' for giving really good clarity and correct answers.\", metadata={}, model='gpt-3.5-turbo', name='ML & DL Assistant+ Retrieval', object='assistant', tools=[RetrievalTool(type='retrieval')])"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "assistant = client.beta.assistants.create(\n",
        "  name=name + \"+ Retrieval\",\n",
        "  instructions=instructions,\n",
        "  model=model,\n",
        "  tools=[{\"type\": \"retrieval\"}],\n",
        "  file_ids=[file_reference.id]\n",
        ")\n",
        "assistant"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0-mmRjQGeUR"
      },
      "source": [
        "Let's try submitting a message to our Assistant and seeing what kind of answer we get!\n",
        "\n",
        "We'll outline the steps needed to do this in full:\n",
        "\n",
        "1. Create an Assistant\n",
        "2. Create a Thread\n",
        "3. Add Messages to that Thread\n",
        "4. Create a Run on that Thread\n",
        "5. Wait for Run to Complete\n",
        "6. Collect Messages from the Thread\n",
        "\n",
        "Let's do that below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOKtb9PsGiB1",
        "outputId": "043ecfc0-2277-4b39-ba6b-fdf8a741e70f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "queued\n",
            "in_progress\n",
            "in_progress\n",
            "in_progress\n",
            "in_progress\n",
            "in_progress\n"
          ]
        }
      ],
      "source": [
        "# Create a Thread\n",
        "thread = client.beta.threads.create()\n",
        "\n",
        "# Add Messages to that Thread\n",
        "message = client.beta.threads.messages.create(\n",
        "    thread_id=thread.id,\n",
        "    role=\"user\",\n",
        "    content=f\"What is ML?\"\n",
        ")\n",
        "\n",
        "# Create a Run on that Thread\n",
        "run = client.beta.threads.runs.create(\n",
        "  thread_id=thread.id,\n",
        "  assistant_id=assistant.id,\n",
        ")\n",
        "\n",
        "# Wait for Run to Complete\n",
        "while run.status == \"in_progress\" or run.status == \"queued\":\n",
        "  time.sleep(1)\n",
        "  print(run.status)\n",
        "  run = client.beta.threads.runs.retrieve(\n",
        "    thread_id=thread.id,\n",
        "    run_id=run.id\n",
        "  )\n",
        "\n",
        "# Collect Messages from the Thread\n",
        "messages = client.beta.threads.messages.list(\n",
        "  thread_id=thread.id\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxJfa-saHbNQ"
      },
      "source": [
        "Let's look at the final result!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19nDqjRgHaNA",
        "outputId": "f4ad3574-2056-4253-d406-f7057ef73dae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Message(id='msg_bvhYbHCenLxUhUAPUHklBDu7', assistant_id='asst_3FIxHxogEnep6NAnJw2vbukr', completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value='Machine learning (ML) is a field that focuses on enabling computers to learn from data and improve their performance on specific tasks without being explicitly programmed. In essence, ML algorithms learn from example data to automate the process of building analytical models. This capacity to learn from training data enables computers to identify hidden insights and complex patterns without human intervention. ML has been applied successfully in various domains such as fraud detection, credit scoring, image and speech recognition, natural language processing, and many more. \\n\\nThere are three main types of machine learning:\\n\\n1. Supervised learning: In supervised learning, the algorithm learns patterns from labeled training data, where the input data is paired with the correct output. It is commonly used in applications where the goal is to predict an outcome based on input features.\\n\\n2. Unsupervised learning: In unsupervised learning, the algorithm learns patterns from unlabeled data, seeking to explore the data and find hidden structures or relationships. Clustering and dimensionality reduction are common tasks in unsupervised learning.\\n\\n3. Reinforcement learning: Reinforcement learning involves an agent that learns to make decisions by interacting with an environment. The agent receives feedback in the form of rewards or punishments based on its actions, allowing it to learn the best strategies for a given task.\\n\\nOverall, machine learning plays a crucial role in the development of intelligent systems and has significantly contributed to the advancement of artificial intelligence technologies in various fields.'), type='text')], created_at=1713104362, file_ids=[], incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='assistant', run_id='run_qznrDyDmVOmUY3Gser1rrMqJ', status=None, thread_id='thread_OFXIJW0Ic1EcVHluu6n5f81l'), Message(id='msg_EwemdGYUbm1CN06FxASu5pIl', assistant_id=None, completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value='What is ML?'), type='text')], created_at=1713104360, file_ids=[], incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='user', run_id=None, status=None, thread_id='thread_OFXIJW0Ic1EcVHluu6n5f81l')]\n",
            "2\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Message(id='msg_bvhYbHCenLxUhUAPUHklBDu7', assistant_id='asst_3FIxHxogEnep6NAnJw2vbukr', completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value='Machine learning (ML) is a field that focuses on enabling computers to learn from data and improve their performance on specific tasks without being explicitly programmed. In essence, ML algorithms learn from example data to automate the process of building analytical models. This capacity to learn from training data enables computers to identify hidden insights and complex patterns without human intervention. ML has been applied successfully in various domains such as fraud detection, credit scoring, image and speech recognition, natural language processing, and many more. \\n\\nThere are three main types of machine learning:\\n\\n1. Supervised learning: In supervised learning, the algorithm learns patterns from labeled training data, where the input data is paired with the correct output. It is commonly used in applications where the goal is to predict an outcome based on input features.\\n\\n2. Unsupervised learning: In unsupervised learning, the algorithm learns patterns from unlabeled data, seeking to explore the data and find hidden structures or relationships. Clustering and dimensionality reduction are common tasks in unsupervised learning.\\n\\n3. Reinforcement learning: Reinforcement learning involves an agent that learns to make decisions by interacting with an environment. The agent receives feedback in the form of rewards or punishments based on its actions, allowing it to learn the best strategies for a given task.\\n\\nOverall, machine learning plays a crucial role in the development of intelligent systems and has significantly contributed to the advancement of artificial intelligence technologies in various fields.'), type='text')], created_at=1713104362, file_ids=[], incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='assistant', run_id='run_qznrDyDmVOmUY3Gser1rrMqJ', status=None, thread_id='thread_OFXIJW0Ic1EcVHluu6n5f81l')"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(messages.data)\n",
        "print(len(messages.data))\n",
        "messages.data[-2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JdzJDvmHyNF"
      },
      "source": [
        "Let's do some clean up to make sure we're not being charged anything extra by deleting our resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "01EYWyWBHaoC"
      },
      "outputs": [],
      "source": [
        "file_deletion_status = client.beta.assistants.files.delete(\n",
        "  assistant_id=assistant.id,\n",
        "  file_id=file_reference.id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "FileDeleteResponse(id='file-RiqjOiU8HHACTiJMyYFfWkrU', deleted=True, object='assistant.file.deleted')"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "file_deletion_status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Assistant(id='asst_McpTkpcakxbTjjPldMRudLFG', created_at=1713111965, description=None, file_ids=[], instructions=\"You are an expert assistant answering technical questions on machine learning and deep learning subject.\\nEnsure that your response is unbiased and generic, you will be 'AWARDED' for giving really good clarity and correct answers.\", metadata={}, model='gpt-3.5-turbo', name='ML & DL Assistant + All Tools', object='assistant', tools=[CodeInterpreterTool(type='code_interpreter'), RetrievalTool(type='retrieval'), FunctionTool(function=FunctionDefinition(name='duckduckgo_search', description='Answer non-technical questions. ', parameters={'type': 'object', 'properties': {'query': {'type:': 'string', 'description': \"The search query to use. For example: 'Who is the current Goalie of the Colorado Avalance?'\"}}, 'required': ['query']}), type='function')]), Assistant(id='asst_qaPLg2lbWOW3ROjVCq8yUYu0', created_at=1713108056, description=None, file_ids=[], instructions=\"You are an expert assistant answering technical questions on machine learning and deep learning subject.\\nEnsure that your response is unbiased and generic, you will be 'AWARDED' for giving really good clarity and correct answers.\", metadata={}, model='gpt-3.5-turbo', name='ML & DL Assistant + Function Calling API', object='assistant', tools=[FunctionTool(function=FunctionDefinition(name='duckduckgo_search', description='Answer non-technical questions. ', parameters={'type': 'object', 'properties': {'query': {'type:': 'string', 'description': \"The search query to use. For example: 'Who is the current Goalie of the Colorado Avalance?'\"}}, 'required': ['query']}), type='function')])]\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "# check assisstants, assisstants files\n",
        "\n",
        "my_assistants = client.beta.assistants.list(\n",
        "    order=\"desc\",\n",
        ")\n",
        "print(my_assistants.data)\n",
        "print(len(my_assistants.data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'assistant_files = client.beta.assistants.files.list(\\n  assistant_id=\"asst_qaPLg2lbWOW3ROjVCq8yUYu0\"\\n)\\nprint(assistant_files)'"
            ]
          },
          "execution_count": 220,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "'''assistant_files = client.beta.assistants.files.list(\n",
        "  assistant_id=\"asst_qaPLg2lbWOW3ROjVCq8yUYu0\"\n",
        ")\n",
        "print(assistant_files)'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 221,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'response = client.beta.assistants.delete(\"asst_nlEfM5YuLVwTooMfdfuU2lYB\")\\nprint(response)'"
            ]
          },
          "execution_count": 221,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''response = client.beta.assistants.delete(\"asst_nlEfM5YuLVwTooMfdfuU2lYB\")\n",
        "print(response)'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pln9uYoJICno"
      },
      "source": [
        "### Creating an Assistant with the Code Interpreter Tool\n",
        "\n",
        "Now that we've explored the Retrieval Tool - let's try the Code Interpreter tool!\n",
        "\n",
        "The process will be almost exactly the same - but we can explore a different query, and we'll add our file at the Message level!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "IC81y_VtH9lw"
      },
      "outputs": [],
      "source": [
        "assistant = client.beta.assistants.create(\n",
        "  name=name + \" + Code Interpreter\",\n",
        "  instructions=instructions,\n",
        "  model=model,\n",
        "  tools=[{\"type\": \"code_interpreter\"}],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJPHAJCQJbgi"
      },
      "source": [
        "In the following example, we'll also see how we can package the Thread creation with the Message adding step!\n",
        "\n",
        "> NOTE: Files added at the message/thread level will not be available to the Assistant outside of that Thread."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "FileObject(id='file-RiqjOiU8HHACTiJMyYFfWkrU', bytes=406660, created_at=1713069007, filename='2104.05314.pdf', object='file', purpose='assistants', status='processed', status_details=None)"
            ]
          },
          "execution_count": 131,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "file_reference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "5xVdjH6EJQrr"
      },
      "outputs": [],
      "source": [
        "thread = client.beta.threads.create(\n",
        "  messages=[\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"What kind of file is this?\",\n",
        "      \"file_ids\": [file_reference.id]\n",
        "    }\n",
        "  ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiIYut_dJ0Cv"
      },
      "source": [
        "> NOTE: Remember that we create runs at the *thread* level - and so don't need the message object to continue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ES8laUe_Jwp_",
        "outputId": "7a9b004f-d824-4cca-f21f-bbbd9a55c4c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "queued\n",
            "in_progress\n",
            "in_progress\n",
            "in_progress\n",
            "in_progress\n",
            "in_progress\n",
            "in_progress\n",
            "in_progress\n",
            "in_progress\n",
            "in_progress\n",
            "in_progress\n",
            "in_progress\n",
            "in_progress\n",
            "in_progress\n",
            "in_progress\n",
            "in_progress\n"
          ]
        }
      ],
      "source": [
        "# Create a Run on that Thread\n",
        "run = client.beta.threads.runs.create(\n",
        "  thread_id=thread.id,\n",
        "  assistant_id=assistant.id,\n",
        ")\n",
        "\n",
        "# Wait for Run to Complete\n",
        "while run.status == \"in_progress\" or run.status == \"queued\":\n",
        "  time.sleep(1)\n",
        "  print(run.status)\n",
        "  run = client.beta.threads.runs.retrieve(\n",
        "    thread_id=thread.id,\n",
        "    run_id=run.id\n",
        "  )\n",
        "\n",
        "# Collect Messages from the Thread\n",
        "messages = client.beta.threads.messages.list(\n",
        "  thread_id=thread.id\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYPpGJz5KoDf"
      },
      "source": [
        "We can check the specific steps that the Code Interpreter ran to figure out what steps the Assistant took!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "or7iJ492KI2P"
      },
      "outputs": [],
      "source": [
        "run_steps = client.beta.threads.runs.steps.list(\n",
        "  thread_id=thread.id,\n",
        "  run_id=run.id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwbzExbmKJ4N",
        "outputId": "da00f537-adda-46ac-9a98-79478751e28f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MessageCreationStepDetails(message_creation=MessageCreation(message_id='msg_aGs2G5hVPx7ejj55Dfj95iUm'), type='message_creation')\n",
            "ToolCallsStepDetails(tool_calls=[CodeInterpreterToolCall(id='call_KJYiOSjuAuc9RJv4UDIczKgI', code_interpreter=CodeInterpreter(input=\"# Read the file as bytes to understand its content\\r\\nwith open(file_path, 'rb') as file:\\r\\n    file_bytes = file.read()\\r\\n\\r\\n# Display the first 50 bytes of the file\\r\\nfile_bytes[:50]\", outputs=[CodeInterpreterOutputLogs(logs=\"b'%PDF-1.4\\\\r%\\\\xe2\\\\xe3\\\\xcf\\\\xd3\\\\r\\\\n500 0 obj\\\\r<</Linearized 1/L 406660'\", type='logs')]), type='code_interpreter')], type='tool_calls')\n",
            "MessageCreationStepDetails(message_creation=MessageCreation(message_id='msg_DEnZwtCfUhBAsUxN9wkfgCP2'), type='message_creation')\n",
            "ToolCallsStepDetails(tool_calls=[CodeInterpreterToolCall(id='call_mHuOahusj6v1XYFlxVgiD5XM', code_interpreter=CodeInterpreter(input=\"# Let's read the first few lines of the file to understand its content\\r\\nwith open(file_path, 'r') as file:\\r\\n    first_few_lines = [next(file) for _ in range(5)]\\r\\n\\r\\nfirst_few_lines\", outputs=[CodeInterpreterOutputLogs(logs=\"---------------------------------------------------------------------------\\nUnicodeDecodeError                        Traceback (most recent call last)\\nCell In[4], line 3\\n      1 # Let's read the first few lines of the file to understand its content\\n      2 with open(file_path, 'r') as file:\\n----> 3     first_few_lines = [next(file) for _ in range(5)]\\n      5 first_few_lines\\n\\nCell In[4], line 3, in <listcomp>(.0)\\n      1 # Let's read the first few lines of the file to understand its content\\n      2 with open(file_path, 'r') as file:\\n----> 3     first_few_lines = [next(file) for _ in range(5)]\\n      5 first_few_lines\\n\\nFile <frozen codecs>:322, in decode(self, input, final)\\n\\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xe2 in position 10: invalid continuation byte\\n\", type='logs')]), type='code_interpreter')], type='tool_calls')\n",
            "MessageCreationStepDetails(message_creation=MessageCreation(message_id='msg_n8TjYtAq6ocVVeLoEefjM6Sh'), type='message_creation')\n",
            "ToolCallsStepDetails(tool_calls=[CodeInterpreterToolCall(id='call_VOaD3zHNRF4283GuKfyMqCSr', code_interpreter=CodeInterpreter(input=\"import mimetypes\\r\\n\\r\\n# Redefine the file path\\r\\nfile_path = '/mnt/data/file-RiqjOiU8HHACTiJMyYFfWkrU'\\r\\n\\r\\n# Determine the file type using mimetypes module\\r\\nmimetype, _ = mimetypes.guess_type(file_path)\\r\\n\\r\\nmimetype\", outputs=[CodeInterpreterOutputLogs(logs='', type='logs')]), type='code_interpreter')], type='tool_calls')\n",
            "MessageCreationStepDetails(message_creation=MessageCreation(message_id='msg_mN15UmkleWFgCUGKfyAB1tpz'), type='message_creation')\n",
            "ToolCallsStepDetails(tool_calls=[CodeInterpreterToolCall(id='call_CfNMJgzWNuSAk3bC4b8EM7Mf', code_interpreter=CodeInterpreter(input='import mimetypes\\r\\n\\r\\n# Determine the file type using mimetypes module\\r\\nmimetype, _ = mimetypes.guess_type(file_path)\\r\\n\\r\\nmimetype', outputs=[CodeInterpreterOutputLogs(logs=\"---------------------------------------------------------------------------\\nNameError                                 Traceback (most recent call last)\\nCell In[2], line 4\\n      1 import mimetypes\\n      3 # Determine the file type using mimetypes module\\n----> 4 mimetype, _ = mimetypes.guess_type(file_path)\\n      6 mimetype\\n\\nNameError: name 'file_path' is not defined\\n\", type='logs')]), type='code_interpreter')], type='tool_calls')\n",
            "MessageCreationStepDetails(message_creation=MessageCreation(message_id='msg_8RBxsOIGaFWGzcApFoLcYW9l'), type='message_creation')\n",
            "ToolCallsStepDetails(tool_calls=[CodeInterpreterToolCall(id='call_9kCgnU8mQ6rcbnONXgzUkUMW', code_interpreter=CodeInterpreter(input=\"import magic\\r\\n\\r\\n# Function to determine the file type\\r\\ndef determine_file_type(file_path):\\r\\n    return magic.Magic(mime=True).from_file(file_path)\\r\\n\\r\\n# Path to the uploaded file\\r\\nfile_path = '/mnt/data/file-RiqjOiU8HHACTiJMyYFfWkrU'\\r\\n\\r\\n# Determine the file type\\r\\nfile_type = determine_file_type(file_path)\\r\\nfile_type\\r\\n\", outputs=[CodeInterpreterOutputLogs(logs=\"---------------------------------------------------------------------------\\nModuleNotFoundError                       Traceback (most recent call last)\\nCell In[1], line 1\\n----> 1 import magic\\n      3 # Function to determine the file type\\n      4 def determine_file_type(file_path):\\n\\nModuleNotFoundError: No module named 'magic'\\n\", type='logs')]), type='code_interpreter')], type='tool_calls')\n",
            "MessageCreationStepDetails(message_creation=MessageCreation(message_id='msg_hELrUhzUhU06VaWrGMheyDON'), type='message_creation')\n"
          ]
        }
      ],
      "source": [
        "for step in run_steps.data:\n",
        "  print(step.step_details)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNJeFF6JJ62H",
        "outputId": "d7565eca-5f49-429d-9d4a-220e22d746c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Message(id='msg_aGs2G5hVPx7ejj55Dfj95iUm', assistant_id='asst_nlEfM5YuLVwTooMfdfuU2lYB', completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value='The uploaded file appears to be a PDF file based on the content read. The presence of `%PDF-1.4` at the beginning of the file indicates that it is a PDF document. If you have any specific questions or tasks related to this PDF file or if you need assistance with anything else, please feel free to let me know!'), type='text')], created_at=1713105800, file_ids=[], incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='assistant', run_id='run_lABqxQpzoRzvXrUrDiM9f3IX', status=None, thread_id='thread_SRcJDbGVih5YK8cTauOZzrzH'), Message(id='msg_DEnZwtCfUhBAsUxN9wkfgCP2', assistant_id='asst_nlEfM5YuLVwTooMfdfuU2lYB', completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value='It seems that there was an error in reading the file directly as it might not be a text file. Let me try to open the file and read its contents in bytes to get a better understanding of its type.'), type='text')], created_at=1713105797, file_ids=[], incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='assistant', run_id='run_lABqxQpzoRzvXrUrDiM9f3IX', status=None, thread_id='thread_SRcJDbGVih5YK8cTauOZzrzH'), Message(id='msg_n8TjYtAq6ocVVeLoEefjM6Sh', assistant_id='asst_nlEfM5YuLVwTooMfdfuU2lYB', completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value='The file type of the uploaded file is not clearly identified using the `mimetypes` module. Let me try to read the file to get more information about its content.'), type='text')], created_at=1713105794, file_ids=[], incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='assistant', run_id='run_lABqxQpzoRzvXrUrDiM9f3IX', status=None, thread_id='thread_SRcJDbGVih5YK8cTauOZzrzH'), Message(id='msg_mN15UmkleWFgCUGKfyAB1tpz', assistant_id='asst_nlEfM5YuLVwTooMfdfuU2lYB', completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value='It seems that there was an error. Let me redefine the file path and try to identify the file type again.'), type='text')], created_at=1713105791, file_ids=[], incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='assistant', run_id='run_lABqxQpzoRzvXrUrDiM9f3IX', status=None, thread_id='thread_SRcJDbGVih5YK8cTauOZzrzH'), Message(id='msg_8RBxsOIGaFWGzcApFoLcYW9l', assistant_id='asst_nlEfM5YuLVwTooMfdfuU2lYB', completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value='It seems that the library I intended to use to identify the file type is not available in this environment. Let me try another method to identify the file type.'), type='text')], created_at=1713105789, file_ids=[], incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='assistant', run_id='run_lABqxQpzoRzvXrUrDiM9f3IX', status=None, thread_id='thread_SRcJDbGVih5YK8cTauOZzrzH'), Message(id='msg_hELrUhzUhU06VaWrGMheyDON', assistant_id='asst_nlEfM5YuLVwTooMfdfuU2lYB', completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value=\"Let's first identify the type of file that was uploaded by examining its contents. I will read the file to determine its type.\"), type='text')], created_at=1713105784, file_ids=[], incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='assistant', run_id='run_lABqxQpzoRzvXrUrDiM9f3IX', status=None, thread_id='thread_SRcJDbGVih5YK8cTauOZzrzH'), Message(id='msg_oworek1KJZT5ZcbSwEV0MfS0', assistant_id=None, completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value='What kind of file is this?'), type='text')], created_at=1713105653, file_ids=['file-RiqjOiU8HHACTiJMyYFfWkrU'], incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='user', run_id=None, status=None, thread_id='thread_SRcJDbGVih5YK8cTauOZzrzH')]\n",
            "What kind of file is this?\n",
            "Let's first identify the type of file that was uploaded by examining its contents. I will read the file to determine its type.\n",
            "It seems that the library I intended to use to identify the file type is not available in this environment. Let me try another method to identify the file type.\n",
            "It seems that there was an error. Let me redefine the file path and try to identify the file type again.\n",
            "The file type of the uploaded file is not clearly identified using the `mimetypes` module. Let me try to read the file to get more information about its content.\n",
            "It seems that there was an error in reading the file directly as it might not be a text file. Let me try to open the file and read its contents in bytes to get a better understanding of its type.\n",
            "The uploaded file appears to be a PDF file based on the content read. The presence of `%PDF-1.4` at the beginning of the file indicates that it is a PDF document. If you have any specific questions or tasks related to this PDF file or if you need assistance with anything else, please feel free to let me know!\n"
          ]
        }
      ],
      "source": [
        "print(messages.data)\n",
        "for i in reversed(range(len(messages.data))):\n",
        "    print(messages.data[i].content[0].text.value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'file-RiqjOiU8HHACTiJMyYFfWkrU'"
            ]
          },
          "execution_count": 132,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "file_reference.id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdLC0rsvR5m5"
      },
      "outputs": [],
      "source": [
        "file_deletion_status = client.beta.assistants.files.delete(\n",
        "  assistant_id=assistant.id,\n",
        "  file_id=file_reference.id\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hi10hON2LQmc"
      },
      "source": [
        "And there you go!\n",
        "\n",
        "We've fit our Assistant with an awesome Code Interpreter that lets our Assistant run code on our provided files!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdJxt77oLzu7"
      },
      "source": [
        "### Creating an Assistant with a Function Calling Tool\n",
        "\n",
        "Let's finally create an Assistant that utilizes the Function Calling API.\n",
        "\n",
        "We'll start by creating a function that we wish to be called.\n",
        "\n",
        "We'll utilize DuckDuckGo search to allow our Assistant to have the most up to date information!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5eKEC2wMMVI",
        "outputId": "8077815f-153d-4e9f-c315-74c12035bde7"
      },
      "outputs": [],
      "source": [
        "!pip install -qU duckduckgo_search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "YPFZ_Uq_LawH"
      },
      "outputs": [],
      "source": [
        "from duckduckgo_search import DDGS\n",
        "\n",
        "def duckduckgo_search(query):\n",
        "  with DDGS() as ddgs:\n",
        "    results = [r for r in ddgs.text(query, max_results=5)]\n",
        "    return \"\\n\".join(result[\"body\"] for result in results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-o1TBFpMSvR"
      },
      "source": [
        "Let's test our function to make sure it behaves as we expect it to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "mCUr9jFCMWBw",
        "outputId": "368ab562-a1f1-4660-d07b-4e597cdc9fac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Close. The official 2023 - 2024 roster of the Winnipeg Jets, including position, height, weight, date of birth, age, and birth place. \\nAdam Lowry, who has been a Jet since 2011 when he was drafted 67th overall, is the new captain of the NHL team — its third since relocating to Winnipeg from Atlanta in 2011. Andrew Ladd served ... \\nLowry will follow Andrew Ladd and Blake Wheeler to serve as the third captain of the new Winnipeg Jets franchise. - Sep 12, 2023. After a season without a captain, the Winnipeg Jets have named ... \\nWINNIPEG — The pride in Adam Lowry's voice was evident after being named captain of the Winnipeg Jets on Tuesday. Lowry is the third Jets captain since the team moved from Atlanta in 2011. After going without a captain for the 2022-23 season, Winnipeg chose the rugged centre over alternate captains Josh Morrissey and Mark Scheifele to succeed ... \\nAdam Lowry was named captain of the Winnipeg Jets on Tuesday. The 30-year-old forward was selected by the Jets in the third round (No. 67) of the 2011 NHL Draft and has played his entire nine ...\""
            ]
          },
          "execution_count": 142,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "duckduckgo_search(\"Who is the current captain of the Winnipeg Jets?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzE1nxt5Mi80"
      },
      "source": [
        "Now we need to express how our function works in a way that is compatible with the OpenAI Function Calling API.\n",
        "\n",
        "We'll want to provide a `JSON` object that includes what parameters we have, how to call them, and a short natural language description."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "8ElrWvBnMY_s"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'name': 'duckduckgo_search',\n",
              " 'description': 'Answer non-technical questions. ',\n",
              " 'parameters': {'type': 'object',\n",
              "  'properties': {'query': {'type:': 'string',\n",
              "    'description': \"The search query to use. For example: 'Who is the current Goalie of the Colorado Avalance?'\"}},\n",
              "  'required': ['query']}}"
            ]
          },
          "execution_count": 144,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ddg_function = {\n",
        "    \"name\" : \"duckduckgo_search\",\n",
        "    \"description\" : \"Answer non-technical questions. \",\n",
        "    \"parameters\" : {\n",
        "        \"type\" : \"object\",\n",
        "        \"properties\" : {\n",
        "            \"query\" : {\n",
        "                \"type:\" : \"string\",\n",
        "                \"description\" : \"The search query to use. For example: 'Who is the current Goalie of the Colorado Avalance?'\"\n",
        "            }\n",
        "        },\n",
        "        \"required\" : [\"query\"]\n",
        "    }\n",
        "}\n",
        "ddg_function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyRmJgEQVuGs"
      },
      "source": [
        "####❓ Question\n",
        "\n",
        "Why does the description key-value pair matter?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***ANSWER***\n",
        " - Description helps model to decide when/which tool to use during function calling, it helps model to take action based on reasoning (desc helps here)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tir4WySGM0x2"
      },
      "source": [
        "Now when we create our Assistant - we'll want to include the function description as a tool using the following format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "4eFpwi12Mzlg"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Assistant(id='asst_qaPLg2lbWOW3ROjVCq8yUYu0', created_at=1713108056, description=None, file_ids=[], instructions=\"You are an expert assistant answering technical questions on machine learning and deep learning subject.\\nEnsure that your response is unbiased and generic, you will be 'AWARDED' for giving really good clarity and correct answers.\", metadata={}, model='gpt-3.5-turbo', name='ML & DL Assistant + Function Calling API', object='assistant', tools=[FunctionTool(function=FunctionDefinition(name='duckduckgo_search', description='Answer non-technical questions. ', parameters={'type': 'object', 'properties': {'query': {'type:': 'string', 'description': \"The search query to use. For example: 'Who is the current Goalie of the Colorado Avalance?'\"}}, 'required': ['query']}), type='function')])"
            ]
          },
          "execution_count": 145,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "assistant = client.beta.assistants.create(\n",
        "    name=name + \" + Function Calling API\",\n",
        "    instructions=instructions,\n",
        "    tools=[\n",
        "        {\"type\": \"function\",\n",
        "         \"function\" : ddg_function\n",
        "        }\n",
        "    ],\n",
        "    model=model\n",
        ")\n",
        "assistant"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJCLvWZzNIXR"
      },
      "source": [
        "We need to make a few modifications to our Assistant to include the ability to make calls to our local function and pass the results back to our Assistant for further generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "XHRfvGJ_NQnF"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def wait_for_run_completion(thread_id, run_id):\n",
        "    while True:\n",
        "        time.sleep(1)\n",
        "        run = client.beta.threads.runs.retrieve(thread_id=thread_id, run_id=run_id)\n",
        "        print(f\"Current run status: {run.status}\")\n",
        "        if run.status in ['completed', 'failed', 'requires_action']:\n",
        "            return run\n",
        "\n",
        "def submit_tool_outputs(thread_id, run_id, tools_to_call):\n",
        "    tool_output_array = []\n",
        "    for tool in tools_to_call:\n",
        "        output = None\n",
        "        tool_call_id = tool.id\n",
        "        function_name = tool.function.name\n",
        "        function_args = tool.function.arguments\n",
        "\n",
        "        if function_name == \"duckduckgo_search\":\n",
        "            print(\"Consulting Duck Duck Go...\")\n",
        "            output = duckduckgo_search(query=json.loads(function_args)[\"query\"])\n",
        "\n",
        "        if output:\n",
        "            tool_output_array.append({\"tool_call_id\": tool_call_id, \"output\": output})\n",
        "\n",
        "    print(tool_output_array)\n",
        "\n",
        "    return client.beta.threads.runs.submit_tool_outputs(\n",
        "        thread_id=thread_id,\n",
        "        run_id=run_id,\n",
        "        tool_outputs=tool_output_array\n",
        "    )\n",
        "\n",
        "def print_messages_from_thread(thread_id):\n",
        "    messages = client.beta.threads.messages.list(thread_id=thread_id)\n",
        "    for msg in messages:\n",
        "        print(f\"{msg.role}: {msg.content[0].text.value}\")\n",
        "\n",
        "def use_assistant(query, assistant_id, thread_id=None):\n",
        "  thread = client.beta.threads.create()\n",
        "\n",
        "  message = client.beta.threads.messages.create(\n",
        "      thread_id=thread.id,\n",
        "      role=\"user\",\n",
        "      content=query,\n",
        "  )\n",
        "\n",
        "  print(\"Creating Assistant \")\n",
        "\n",
        "  run = client.beta.threads.runs.create(\n",
        "    thread_id=thread.id,\n",
        "    assistant_id=assistant_id,\n",
        "  )\n",
        "\n",
        "  print(\"Querying OpenAI Assistant Thread.\")\n",
        "\n",
        "  run = wait_for_run_completion(thread.id, run.id)\n",
        "\n",
        "  if run.status == 'requires_action':\n",
        "    run = submit_tool_outputs(thread.id, run.id, run.required_action.submit_tool_outputs.tool_calls)\n",
        "    run = wait_for_run_completion(thread.id, run.id)\n",
        "\n",
        "  print_messages_from_thread(thread.id)\n",
        "\n",
        "  return thread.id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoChJ771Uo0B"
      },
      "source": [
        "####❓ Question\n",
        "\n",
        "Outline, in simple terms, what the `use_assistant` helper function is doing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***ANSWER***\n",
        "\n",
        "- This functions helps to create new thread, create and add messages to thread, helps thread to run, get messages from thread, also function helps to use tools using function calling.\n",
        "- Functions return thread id."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Assistant(id='asst_qaPLg2lbWOW3ROjVCq8yUYu0', created_at=1713108056, description=None, file_ids=[], instructions=\"You are an expert assistant answering technical questions on machine learning and deep learning subject.\\nEnsure that your response is unbiased and generic, you will be 'AWARDED' for giving really good clarity and correct answers.\", metadata={}, model='gpt-3.5-turbo', name='ML & DL Assistant + Function Calling API', object='assistant', tools=[FunctionTool(function=FunctionDefinition(name='duckduckgo_search', description='Answer non-technical questions. ', parameters={'type': 'object', 'properties': {'query': {'type:': 'string', 'description': \"The search query to use. For example: 'Who is the current Goalie of the Colorado Avalance?'\"}}, 'required': ['query']}), type='function')])"
            ]
          },
          "execution_count": 147,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "assistant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5NR_HYINky8",
        "outputId": "60042cd6-117f-423e-9f75-5ea1d663c602"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating Assistant \n",
            "Querying OpenAI Assistant Thread.\n",
            "Current run status: requires_action\n",
            "Consulting Duck Duck Go...\n",
            "[{'tool_call_id': 'call_OJve0R3nbfCxwVODnwitm9Im', 'output': 'Lowry has been a part of the Jets organization since June 25, 2011, when he was selected in the third round, 67 th overall, after putting up 37 points in 36 games with the Swift Current Broncos of ... \\nLowry will follow Andrew Ladd and Blake Wheeler to serve as the third captain of the new Winnipeg Jets franchise. - Sep 12, 2023. After a season without a captain, the Winnipeg Jets have named ... \\nNational team. Canada. NHL Draft. 67th overall, 2011. Winnipeg Jets. Playing career. 2013-present. Adam Lowry (born March 29, 1993) is an American-born Canadian professional ice hockey centre and captain of the Winnipeg Jets of the National Hockey League (NHL). \\nThe Winnipeg Jets have a new leader, one year after stripping the C from Blake Wheeler and deciding to play without a captain. Adam Lowry, who has been a Jet since 2011 when he was drafted 67th ... \\nAdam Lowry was named captain of the Winnipeg Jets on Tuesday. The 30-year-old forward was selected by the Jets in the third round (No. 67) of the 2011 NHL Draft and has played his entire nine ...'}]\n",
            "Current run status: in_progress\n",
            "Current run status: completed\n",
            "assistant: The current Captain of the Winnipeg Jets is Adam Lowry. He was named the captain of the team on September 12, 2023, and has been a part of the Jets organization since June 25, 2011.\n",
            "user: Who is the current Captain of the Winnipeg Jets?\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'thread_v16eZOo0rCY161hZ5uIjCAJh'"
            ]
          },
          "execution_count": 148,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "use_assistant(\"Who is the current Captain of the Winnipeg Jets?\", assistant.id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY51QtkGNvQe"
      },
      "source": [
        "## Wrapping it All Together\n",
        "\n",
        "Now we can create an Assistant with all of the available tools and see how it responds to various queries!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "FileObject(id='file-RiqjOiU8HHACTiJMyYFfWkrU', bytes=406660, created_at=1713069007, filename='2104.05314.pdf', object='file', purpose='assistants', status='processed', status_details=None)"
            ]
          },
          "execution_count": 152,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "file_reference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "HaoQSB7CN74T"
      },
      "outputs": [],
      "source": [
        "assistant = client.beta.assistants.create(\n",
        "    name=name + \" + All Tools\",\n",
        "    instructions=instructions,\n",
        "    tools=[\n",
        "        {\"type\": \"code_interpreter\"},\n",
        "        {\"type\": \"retrieval\"},\n",
        "        {\"type\": \"function\", \"function\" : ddg_function}\n",
        "    ],\n",
        "    model=model,\n",
        "    file_ids=[file_reference.id],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Assistant(id='asst_McpTkpcakxbTjjPldMRudLFG', created_at=1713111965, description=None, file_ids=['file-RiqjOiU8HHACTiJMyYFfWkrU'], instructions=\"You are an expert assistant answering technical questions on machine learning and deep learning subject.\\nEnsure that your response is unbiased and generic, you will be 'AWARDED' for giving really good clarity and correct answers.\", metadata={}, model='gpt-3.5-turbo', name='ML & DL Assistant + All Tools', object='assistant', tools=[CodeInterpreterTool(type='code_interpreter'), RetrievalTool(type='retrieval'), FunctionTool(function=FunctionDefinition(name='duckduckgo_search', description='Answer non-technical questions. ', parameters={'type': 'object', 'properties': {'query': {'type:': 'string', 'description': \"The search query to use. For example: 'Who is the current Goalie of the Colorado Avalance?'\"}}, 'required': ['query']}), type='function')]),\n",
              " Assistant(id='asst_qaPLg2lbWOW3ROjVCq8yUYu0', created_at=1713108056, description=None, file_ids=[], instructions=\"You are an expert assistant answering technical questions on machine learning and deep learning subject.\\nEnsure that your response is unbiased and generic, you will be 'AWARDED' for giving really good clarity and correct answers.\", metadata={}, model='gpt-3.5-turbo', name='ML & DL Assistant + Function Calling API', object='assistant', tools=[FunctionTool(function=FunctionDefinition(name='duckduckgo_search', description='Answer non-technical questions. ', parameters={'type': 'object', 'properties': {'query': {'type:': 'string', 'description': \"The search query to use. For example: 'Who is the current Goalie of the Colorado Avalance?'\"}}, 'required': ['query']}), type='function')])]"
            ]
          },
          "execution_count": 156,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "client.beta.assistants.list().data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'asst_McpTkpcakxbTjjPldMRudLFG'"
            ]
          },
          "execution_count": 157,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "assistant.id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2GtwoGDPwp2",
        "outputId": "e276723d-54e1-41ff-8b47-ef725caaabb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating Assistant \n",
            "Querying OpenAI Assistant Thread.\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: completed\n",
            "assistant: The current Captain of the Winnipeg Jets is Blake Wheeler【7:0†source】.\n",
            "user: Who is the current Captain of the Winnipeg Jets?\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'thread_uX0Iforfl1jvZe0hykhMRn91'"
            ]
          },
          "execution_count": 158,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "use_assistant(\"Who is the current Captain of the Winnipeg Jets?\", assistant.id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QjcxpQ5P-HX",
        "outputId": "43f6a9be-ad9b-4177-92bb-6064dbc0d88b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating Assistant \n",
            "Querying OpenAI Assistant Thread.\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: completed\n",
            "assistant: The authors of the supplied file are:\n",
            "\n",
            "1. Christian Janiesch\n",
            "2. Patrick Zschech\n",
            "3. Kai Heinrich\n",
            "\n",
            "They are affiliated with different academic institutions as follows:\n",
            "- Christian Janiesch is from the Faculty of Business Management & Economics, University of Würzburg, Germany.\n",
            "- Patrick Zschech is from the Institute of Information Systems, Friedrich-Alexander University Erlangen-Nürnberg, Germany.\n",
            "- Kai Heinrich is from the Faculty of Economics and Management, Otto-von-Guericke-Universität Magdeburg, Germany【5†source】.\n",
            "user: Who is the author of the supplied file?\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'thread_wak7QimvBHnAcHCxzCR8VSOD'"
            ]
          },
          "execution_count": 159,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "use_assistant(\"Who is the author of the supplied file?\", assistant.id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSKL96nvQIUq",
        "outputId": "e054c800-7c54-4601-d972-4b3c5dc1125d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating Assistant \n",
            "Querying OpenAI Assistant Thread.\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: completed\n",
            "assistant: The provided file is 406,660 bytes in size.\n",
            "user: How many bytes is the provided file?\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'thread_FWv5SfGRNboWeAVDGY42LlPN'"
            ]
          },
          "execution_count": 160,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "use_assistant(\"How many bytes is the provided file?\", assistant.id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating Assistant \n",
            "Querying OpenAI Assistant Thread.\n",
            "Current run status: in_progress\n",
            "Current run status: requires_action\n",
            "Consulting Duck Duck Go...\n",
            "[{'tool_call_id': 'call_jh444hquVjdmsFWpUpHO7cED', 'output': \"Lowry has been a part of the Jets organization since June 25, 2011, when he was selected in the third round, 67 th overall, after putting up 37 points in 36 games with the Swift Current Broncos of ... \\nThe Winnipeg Jets have a new leader, one year after stripping the C from Blake Wheeler and deciding to play without a captain. Adam Lowry, who has been a Jet since 2011 when he was drafted 67th ... \\nLowry will follow Andrew Ladd and Blake Wheeler to serve as the third captain of the new Winnipeg Jets franchise. - Sep 12, 2023. After a season without a captain, the Winnipeg Jets have named ... \\nThe Winnipeg Jets have officially made the decision on which player will wear the captain's 'C' for the 2023-24 season, and hopefully onward. That player is 30-year-old centre Adam Lowry. \\nBy Murat Ates. Sep 13, 2023. 13. Adam Lowry sat between his coach, Rick Bowness, and the GM who drafted him, Kevin Cheveldayoff, in front of an enormous, nine-panel screen at Winnipeg's newly ...\"}]\n",
            "Current run status: completed\n",
            "assistant: The current Captain of the Winnipeg Jets is Adam Lowry .\n",
            "user: Who is the current Captain of the Winnipeg Jets search internet?\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'thread_NVUQrFY6GCTLz7U9U1ZUUf8c'"
            ]
          },
          "execution_count": 164,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "use_assistant(\"Who is the current Captain of the Winnipeg Jets search internet?\", assistant.id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gm0oYJu7VAjg"
      },
      "source": [
        "####❓ Question\n",
        "\n",
        "Notice that our response can go through multiple paths, given that:\n",
        "\n",
        "What is \"deciding\" to use the tool?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***ANSWER***:\n",
        "- Agent/Assisstant will deciode which tool to use through reasoning based on the tools(description) we provide as function.\n",
        "- Assisstant know's what tools it has access to, which we will give when creating Assisstant's. Ig user asks query to give code it knows to code_interpreter, if user asks question from the files it uses retrieval, if user asks any question which requires custom tool which we pass as fucntion, then asssisstant uses that tool using function calling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrJWQ4XvZ20K"
      },
      "source": [
        "### Adding JSON Mode for More Agentic Behaviour\n",
        "\n",
        "Finally, we have the ability to select tools - all we need to do now is set up a process to allow us to create some kind of loop and make decisions about whether or not the response is complete or not.\n",
        "\n",
        "We'll leverage the OpenAI completions end-endpoint with JSON mode to let us understand when we've adequately answered our user's question!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "id": "Rzrk8gn2anpe"
      },
      "outputs": [],
      "source": [
        "completed_template = \\\n",
        "\"\"\"\n",
        "Does this response adequately answer the user's query?\n",
        "\n",
        "Please return your response in JSON format - with key: \"completed\" and either True (if completed) or False (if not completed)\n",
        "\n",
        "User Query:\n",
        "{query}\n",
        "\n",
        "Assistant Response:\n",
        "{response}\n",
        "\"\"\"\n",
        "\n",
        "def is_complete(query, response):\n",
        "  completed_response = client.chat.completions.create(\n",
        "      messages=[\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": completed_template.format(query=query, response=response),\n",
        "          }\n",
        "      ],\n",
        "      model=model,\n",
        "      response_format={\"type\" : \"json_object\"}\n",
        "  )\n",
        "  #print(completed_response)\n",
        "  return completed_response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kCQrx03brge",
        "outputId": "2e2a716d-718f-4e06-bf80-a58922180441"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating Assistant \n",
            "Querying OpenAI Assistant Thread.\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: completed\n",
            "assistant: Machine learning (ML) is a branch of artificial intelligence (AI) that focuses on developing algorithms and techniques that enable computers to learn from and make decisions or predictions based on data. In essence, machine learning allows computers to learn and improve their performance on a task without being explicitly programmed for that task.\n",
            "\n",
            "ML algorithms are designed to analyze data, identify patterns, and make decisions or predictions based on these patterns. There are various types of machine learning approaches, including supervised learning, unsupervised learning, semi-supervised learning, reinforcement learning, and more. Each approach is suited for different types of tasks and data.\n",
            "\n",
            "Supervised learning involves training a model on labeled data, where the algorithm learns to map inputs to outputs. Unsupervised learning, on the other hand, deals with unlabeled data, where the algorithm learns to find patterns or structure in the data. Reinforcement learning involves training agents to make sequential decisions by rewarding or punishing their actions based on the outcomes.\n",
            "\n",
            "Overall, machine learning plays a crucial role in various fields such as image and speech recognition, natural language processing, medical diagnosis, predictive analytics, and more, by enabling computers to learn from data and make intelligent decisions.\n",
            "user: what is ML?\n"
          ]
        }
      ],
      "source": [
        "query = \"what is ML?\"\n",
        "\n",
        "thread_id_for_response = use_assistant(query, assistant.id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSJs9oDIgBTK"
      },
      "source": [
        "Now we can observe JSON mode in action!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "id": "MSDsmlDBcYk_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Machine learning (ML) is a branch of artificial intelligence (AI) that focuses on developing algorithms and techniques that enable computers to learn from and make decisions or predictions based on data. In essence, machine learning allows computers to learn and improve their performance on a task without being explicitly programmed for that task.\n",
            "\n",
            "ML algorithms are designed to analyze data, identify patterns, and make decisions or predictions based on these patterns. There are various types of machine learning approaches, including supervised learning, unsupervised learning, semi-supervised learning, reinforcement learning, and more. Each approach is suited for different types of tasks and data.\n",
            "\n",
            "Supervised learning involves training a model on labeled data, where the algorithm learns to map inputs to outputs. Unsupervised learning, on the other hand, deals with unlabeled data, where the algorithm learns to find patterns or structure in the data. Reinforcement learning involves training agents to make sequential decisions by rewarding or punishing their actions based on the outcomes.\n",
            "\n",
            "Overall, machine learning plays a crucial role in various fields such as image and speech recognition, natural language processing, medical diagnosis, predictive analytics, and more, by enabling computers to learn from data and make intelligent decisions.\n",
            "ChatCompletion(id='chatcmpl-9DxDfuskq4ZGUUvFLnA5FAWzUtoL2', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='{\\n    \"completed\": true\\n}', role='assistant', function_call=None, tool_calls=None))], created=1713112931, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_c2295e73ad', usage=CompletionUsage(completion_tokens=8, prompt_tokens=292, total_tokens=300))\n"
          ]
        }
      ],
      "source": [
        "messages = client.beta.threads.messages.list(thread_id=thread_id_for_response)\n",
        "\n",
        "response = messages.data[0].content[0].text.value\n",
        "print(response)\n",
        "completed_flag = json.loads(is_complete(query, response).choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-SKB6TWf_Ra",
        "outputId": "e2fdb56c-ca8a-4011-ff25-eb55d8dc5d1f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'completed': True}"
            ]
          },
          "execution_count": 184,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "completed_flag"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bivIAVh0dTwA"
      },
      "source": [
        "## 🚧 BONUS CHALLENGE 🚧:\n",
        "\n",
        "Use the components we've constructed so far to build a loop that lets us continue to query the Assistant if the response is not completed!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {
        "id": "vtWh_h_WfjuS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating Assistant \n",
            "Querying OpenAI Assistant Thread.\n",
            "Current run status: in_progress\n",
            "Current run status: completed\n",
            "assistant: I'm not sure what you mean by \"dhage\" in this context. Could you please provide more information or clarify your question so I can assist you better?\n",
            "user: give code of dhage?, try and if you don'y know say don't know after some retries\n",
            "*********\n",
            "thread_v6mLSjNpMQF9ZC1ct23HSyeo\n",
            "ChatCompletion(id='chatcmpl-9DxmP4rNgVEbxx4M7nNtBttRfxrze', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='{\\n  \"completed\": false\\n}', role='assistant', function_call=None, tool_calls=None))], created=1713115085, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_c2295e73ad', usage=CompletionUsage(completion_tokens=8, prompt_tokens=106, total_tokens=114))\n",
            "########################################################\n",
            "{'completed': False}\n",
            "########################################################\n",
            "Creating Assistant \n",
            "Querying OpenAI Assistant Thread.\n",
            "Current run status: in_progress\n",
            "Current run status: completed\n",
            "assistant: I'm sorry, could you please provide more context or details on what you mean by \"dhage\"? It seems to be a term or concept that I'm not familiar with in the context of machine learning or deep learning. If you could give me more information or clarify further, I would be happy to help.\n",
            "user: give code of dhage?, try and if you don'y know say don't know after some retries\n",
            "*********\n",
            "thread_fKK6MyAk4ubgLmBVOTLHKfOD\n",
            "ChatCompletion(id='chatcmpl-9DxmT2Gmbsmy2URHV1InL2HuLmwSD', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='{\\n  \"completed\": false\\n}', role='assistant', function_call=None, tool_calls=None))], created=1713115089, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_c2295e73ad', usage=CompletionUsage(completion_tokens=8, prompt_tokens=136, total_tokens=144))\n",
            "########################################################\n",
            "Completed flag check based on repsonse: False\n",
            "########################################################\n",
            "Creating Assistant \n",
            "Querying OpenAI Assistant Thread.\n",
            "Current run status: completed\n",
            "assistant: I'm not familiar with the term \"dhage\" in the context of machine learning or programming. Could you please provide more context or explain what you mean by \"dhage\" so that I can better assist you?\n",
            "user: give code of dhage?, try and if you don'y know say don't know after some retries\n",
            "*********\n",
            "thread_kLtBz7BKwGpksfcwRFKEgtyh\n",
            "ChatCompletion(id='chatcmpl-9DxmVDhX8ZIp2LqVBMKBpURY7hyWt', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='{\\n  \"completed\": false\\n}', role='assistant', function_call=None, tool_calls=None))], created=1713115091, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_c2295e73ad', usage=CompletionUsage(completion_tokens=8, prompt_tokens=117, total_tokens=125))\n",
            "########################################################\n",
            "Completed flag check based on repsonse: False\n",
            "########################################################\n",
            "Creating Assistant \n",
            "Querying OpenAI Assistant Thread.\n",
            "Current run status: completed\n",
            "assistant: I'm not sure what you mean by \"dhage\" in this context. Could you provide more details or context so I can better assist you?\n",
            "user: give code of dhage?, try and if you don'y know say don't know after some retries\n",
            "*********\n",
            "thread_7Sq3Oiza3xasmbn5IrEqAqUB\n",
            "ChatCompletion(id='chatcmpl-9DxmYvtytBbcSY0bbozgZKkWa8MDj', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='{\\n  \"completed\": true\\n}', role='assistant', function_call=None, tool_calls=None))], created=1713115094, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_c2295e73ad', usage=CompletionUsage(completion_tokens=8, prompt_tokens=103, total_tokens=111))\n",
            "########################################################\n",
            "Completed flag check based on repsonse: True\n",
            "########################################################\n"
          ]
        }
      ],
      "source": [
        "# Experimentiing for bonus challenge\n",
        "query = \"give code of dhage?, try and if you don'y know say don't know after some retries\"\n",
        "\n",
        "thread_id_for_response = use_assistant(query, assistant.id)\n",
        "print(\"*********\")\n",
        "print(thread_id_for_response)\n",
        "messages = client.beta.threads.messages.list(thread_id=thread_id_for_response)\n",
        "response = messages.data[0].content[0].text.value\n",
        "\n",
        "completed_flag = json.loads(is_complete(query, response).choices[0].message.content)\n",
        "print(\"########################################################\")\n",
        "print(completed_flag)\n",
        "print(\"########################################################\")\n",
        "while completed_flag['completed'] == False:\n",
        "    thread_id_for_response = use_assistant(query, assistant.id)\n",
        "    print(\"*********\")\n",
        "    print(thread_id_for_response)\n",
        "    messages = client.beta.threads.messages.list(thread_id=thread_id_for_response)\n",
        "    response = messages.data[0].content[0].text.value\n",
        "    completed_flag = json.loads(is_complete(query, response).choices[0].message.content)\n",
        "    print(\"########################################################\")\n",
        "    print(\"Completed flag check based on repsonse: \" + str(completed_flag['completed']))\n",
        "    print(\"########################################################\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🚧SOLUTION🚧"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating Assistant \n",
            "Querying OpenAI Assistant Thread.\n",
            "Current run status: in_progress\n",
            "Current run status: completed\n",
            "assistant: I'm not sure what you mean by \"hagedfg?\" Could you please provide more context or clarify your question so I can better assist you?\n",
            "user: Give the code of hagedfg?, try and if you don't know say don't know after some retries\n",
            "*********\n",
            "Thread ID for response: thread_SjVGZ2UutUkdxlabu5YM7TEH\n",
            "ChatCompletion(id='chatcmpl-9DyVLG87nbZWTHJ6vnXzCwOgrRZ1X', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='{\\n  \"completed\": false\\n}', role='assistant', function_call=None, tool_calls=None))], created=1713117871, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_c2295e73ad', usage=CompletionUsage(completion_tokens=8, prompt_tokens=105, total_tokens=113))\n",
            "########################################################\n",
            "Completed flag: {'completed': False}\n",
            "########################################################\n",
            "Response not completed, retrying...\n",
            "Creating Assistant \n",
            "Querying OpenAI Assistant Thread.\n",
            "Current run status: in_progress\n",
            "Current run status: in_progress\n",
            "Current run status: completed\n",
            "assistant: The code for \"hagedfg\" does not exist, and therefore, I couldn't find it.\n",
            "user: Give the code of hagedfg?, try and if you don't know say don't know after some retries\n",
            "*********\n",
            "Thread ID for response: thread_OrSqou7eCSQtiblU6Nz74eUs\n",
            "ChatCompletion(id='chatcmpl-9DyVS4ArXEEoSXrdI73Zb9XVLK9od', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='{\\n  \"completed\": true\\n}', role='assistant', function_call=None, tool_calls=None))], created=1713117878, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_c2295e73ad', usage=CompletionUsage(completion_tokens=8, prompt_tokens=96, total_tokens=104))\n",
            "########################################################\n",
            "Completed flag: {'completed': True}\n",
            "########################################################\n",
            "\n",
            "Assistant has successfully completed the response.(🎉(🎉(🎉\n"
          ]
        }
      ],
      "source": [
        "MAX_RETRIES = 5\n",
        "assistant_id = assistant.id\n",
        "\n",
        "query = \"Give the code of hagedfg?, try and if you don't know say don't know after some retries\"\n",
        "\n",
        "def query_assistant_until_complete(query, assistant_id):\n",
        "    retries = 0\n",
        "    while retries < MAX_RETRIES:\n",
        "        thread_id_for_response = use_assistant(query, assistant_id)\n",
        "        print(\"*********\")\n",
        "        print(\"Thread ID for response:\", thread_id_for_response)\n",
        "\n",
        "        messages = client.beta.threads.messages.list(thread_id=thread_id_for_response)\n",
        "        if not messages.data:\n",
        "            print(\"No messages returned.\")\n",
        "            retries += 1\n",
        "            time.sleep(1)  \n",
        "            continue\n",
        "        \n",
        "        response = messages.data[0].content[0].text.value\n",
        "\n",
        "        completed_flag = json.loads(is_complete(query, response).choices[0].message.content)\n",
        "        print(\"########################################################\")\n",
        "        print(\"Completed flag:\", completed_flag)\n",
        "        print(\"########################################################\")\n",
        "\n",
        "        if completed_flag.get('completed', False):\n",
        "            print(\"\")\n",
        "            print(\"Assistant has successfully completed the response.🎉🎉🎉\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"Response not completed, retrying...\")\n",
        "            retries += 1\n",
        "            time.sleep(2) \n",
        "\n",
        "        if retries == MAX_RETRIES:\n",
        "            print(\"Max retries reached without a completed response.\")\n",
        "        \n",
        "    return thread_id_for_response\n",
        "\n",
        "thread_id_for_response = query_assistant_until_complete(query, assistant_id)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJjYm6gnfWrS"
      },
      "source": [
        "# Make Sure You Delete Resources\n",
        "\n",
        "Make sure you delete all the resources you created!\n",
        "\n",
        "This function will help you do so!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "id": "62b49AleR6m_"
      },
      "outputs": [],
      "source": [
        "file_deletion_status = client.beta.assistants.files.delete(\n",
        "  assistant_id=assistant.id,\n",
        "  file_id=file_reference.id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "FileDeleteResponse(id='file-RiqjOiU8HHACTiJMyYFfWkrU', deleted=True, object='assistant.file.deleted')"
            ]
          },
          "execution_count": 176,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "file_deletion_status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
